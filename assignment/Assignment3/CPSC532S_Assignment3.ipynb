{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnzrOsd0bZHS"
   },
   "source": [
    "# CPSC532S Assignment 3:  RNNs for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "t_cnRUY7dkdI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oSmx8s7dkdK",
    "tags": []
   },
   "source": [
    "# Data Acquisition\n",
    "\n",
    "\n",
    "The goal of this assignment is to translate English to Pig Latin. For this assignment, you must download the data and extract it into `data/`. The dataset contains four files, each containing a single caption on each line. There are two files for training (English vs Pig Latin) and two files for validation. We should have 20,000 sentences (one sentence per image in Assignment 2) in the training captions and 500 sentences in the validation captions (five sentences per image in Assignment 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS3DYtiqdkdL",
    "outputId": "b7c2ef71-f89a-401b-b9c2-670dfc0fd92e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/train_captions.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from google.colab import drive\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# drive.mount('/data')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the data into memory.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m mscoco_train \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/train_captions.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m mscoco_val  \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/val_captions.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m mscoco_piglatin_train \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/piglatin_train_captions.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_captions.json'"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/data')\n",
    "\n",
    "# Load the data into memory.\n",
    "mscoco_train = json.load(open(\"/content/train_captions.json\"))\n",
    "mscoco_val  = json.load(open(\"/content/val_captions.json\"))\n",
    "\n",
    "mscoco_piglatin_train = json.load(open('/content/piglatin_train_captions.json'))\n",
    "mscoco_piglatin_val  = json.load(open('/content/piglatin_val_captions.json'))\n",
    "\n",
    "train_sentences = [entry['caption'] for entry in mscoco_train['annotations']]\n",
    "train_sentences = train_sentences[:5000]\n",
    "val_sentences = [entry['caption'] for entry in mscoco_val['annotations']]\n",
    "\n",
    "piglatin_train_sentences = [entry['caption'] for entry in mscoco_piglatin_train['annotations']]\n",
    "piglatin_train_sentences = piglatin_train_sentences[:5000]\n",
    "piglatin_val_sentences = [entry['caption'] for entry in mscoco_piglatin_val['annotations']]\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(piglatin_train_sentences))\n",
    "print(len(piglatin_val_sentences))\n",
    "print(train_sentences[0])\n",
    "print(piglatin_train_sentences[0])\n",
    "print(val_sentences[0])\n",
    "print(piglatin_val_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eH3KLA_NIqZR",
    "outputId": "632775b9-cf14-470b-d781-65fd5df5e1ec"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyGm8VmCdkdM"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "The code provided below creates word embeddings for you to use. After creating the vocabulary, we construct both one-hot embeddings and word2vec embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYWipM4tdkdN",
    "outputId": "c9e42e5d-8fd8-4fcc-8cf2-ff1671a45e7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xuanchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m piglatin_filtered_sentences \u001b[38;5;241m=\u001b[39m [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word2index] \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m piglatin_sentences]\n\u001b[1;32m     27\u001b[0m all_filtered_sentences \u001b[38;5;241m=\u001b[39m filtered_sentences \u001b[38;5;241m+\u001b[39m piglatin_filtered_sentences\n\u001b[0;32m---> 28\u001b[0m w2v \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_filtered_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwordEncodingSize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m w2v_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, wordEncodingSize)), w2v\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Define the max sequence length to be the longest sentence in the training data. \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = train_sentences\n",
    "piglatin_sentences = piglatin_train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "piglatin_sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in piglatin_sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 2000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "piglatin_word_counts = Counter([word for sentence in piglatin_sentences for word in sentence])\n",
    "word_counts = word_counts + piglatin_word_counts\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "\n",
    "# Build the one hot embeddings\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "piglatin_filtered_sentences = [[word for word in sentence if word in word2index] for sentence in piglatin_sentences]\n",
    "all_filtered_sentences = filtered_sentences + piglatin_filtered_sentences\n",
    "w2v = Word2Vec(all_filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.vectors))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "piglatin_maxSequenceLength = max([len(sentence) for sentence in piglatin_sentences])\n",
    "\n",
    "if piglatin_maxSequenceLength > maxSequenceLength:\n",
    "    maxSequenceLength = piglatin_maxSequenceLength\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zYnaPC0i0A2"
   },
   "source": [
    "# Utilities functions\n",
    "\n",
    "\n",
    "Please look through the functions provided below carefully, as you will need to use all of them at some point in your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PFOm2o3jINX",
    "outputId": "06151d19-916e-486e-82b3-7cee77ce31cd"
   },
   "outputs": [],
   "source": [
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentence, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a reference sentence, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = word_tokenize(reference_sentence.lower())\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu([reference_tokenized], predicted_tokenized)\n",
    "\n",
    "%matplotlib inline\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +['<EOS>'])\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "score1 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[0])\n",
    "score2 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[5])\n",
    "\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[0] + '\" \\nis: ' + str(score1) +'\\n\\n')\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[5] + '\" \\nis: ' + str(score2) +'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZD-r_MckSLA"
   },
   "source": [
    "#Part 1: Encoder-Decoder Language Translation with Teacher-Forcing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSOKOJModkdN"
   },
   "source": [
    "## 1.1 Building a Language Decoder\n",
    "\n",
    "We now implement a language decoder. For now, we will have the decoder take a single training sample at a time (as opposed to batching). For our purposes, we will also avoid defining the embeddings as part of the model and instead pass in embedded inputs. While this is sometimes useful, as it learns/tunes the embeddings, we avoid doing it for the sake of simplicity and speed.\n",
    "\n",
    "Remember to use LSTM hidden units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w7g1DsQcdkdO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        \n",
    "        # Your code goes here (~4 lines or less)\n",
    "        self.lstm = nn.LSTM(wordEncodingSize, self.hidden_dim)\n",
    "        self.linear = nn.Linear(self.hidden_dim, wordEncodingSize)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "\n",
    "\n",
    "    def init_cell(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(input_sentence, (hidden, cell))\n",
    "        output = self.softmax(self.linear(output[0]))\n",
    "        return output, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJVTRdUkdkdR"
   },
   "source": [
    "## 1.2.  Building Language Encoder\n",
    "\n",
    "We now build a language encoder, which will encode an input word by word, and ultimately output a hidden state that we can then be used by our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kccifunUdkdR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        \n",
    "        # Your code goes here (~3 lines)\n",
    "        self.lstm = nn.LSTM(input_size=wordEncodingSize, hidden_size=self.hidden_dim)\n",
    "       \n",
    "    def init_hidden(self):\n",
    "        # Your code goes here (1 line)\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "    \n",
    "    def init_cell(self):\n",
    "        # Your code goes here (1 line)\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell):\n",
    "        # Your code goes here (~2 lines of code)\n",
    "        output, (hidden, cell) = self.lstm(input_sentence, (hidden, cell))\n",
    "        \n",
    "        return output, hidden, cell\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9i-rNQ8sGdrN",
    "outputId": "842fe8d3-521a-49be-a35e-b4bf663c1593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 2000)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor(\n",
    "        preprocess_numberize(\"A very clean and well decorated empty bathroom\"),\n",
    "        dtype=torch.long\n",
    "    ).view(-1, 1)\n",
    "encoder = EncoderLSTM()\n",
    "encoder_hidden = encoder.init_hidden()\n",
    "encoder_cell = encoder.init_cell()\n",
    "input_sentence_one_hot_embeddings = one_hot_embeddings[input_tensor]\n",
    "print(input_sentence_one_hot_embeddings.shape)\n",
    "input_length = input_sentence_one_hot_embeddings.shape[0]\n",
    "\n",
    "# for ei in range(input_length):\n",
    "#   encoder_input = torch.tensor(input_sentence_one_hot_embeddings[ei,:], dtype=torch.float).view(1,1,-1)\n",
    "#   encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "# print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQDOBB03UhIP",
    "outputId": "9064aafa-2889-4f9b-d335-a8021d228a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 300])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "test = torch.zeros(maxSequenceLength, 300)\n",
    "print(test.shape)\n",
    "decoder_input = torch.tensor([preprocess_numberize(\"<SOS>\")])\n",
    "print(decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7y9AKd5ePCV",
    "outputId": "a5e521d5-6455-44e5-eafc-be2e07fef43f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2000])\n",
      "tensor([[407]])\n",
      "torch.Size([1, 1, 2000])\n",
      "tensor([[2]])\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderLSTM()\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder_cell = encoder_cell\n",
    "decoder_input = torch.tensor([word2index.get(\"<SOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "decoder_input = one_hot_embeddings[decoder_input]\n",
    "decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "for di in range(input_length):\n",
    "  decoder_output, decoder_hidden, decoder_cell = decoder(\n",
    "      decoder_input, decoder_hidden, decoder_cell\n",
    "  )\n",
    "print(decoder_output.shape)\n",
    "topv, topi = decoder_output.topk(1)\n",
    "decoder_input = topi.squeeze().detach()\n",
    "decoder_input = one_hot_embeddings[decoder_input]\n",
    "decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "print(topi)\n",
    "print(decoder_input.shape)\n",
    "test = torch.tensor([word2index.get(\"<EOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljBX3m0tdkdT"
   },
   "source": [
    "## 1.3. Connecting Encoder to Decoder and Train End-to-End and Train with Teacher Forcing\n",
    "\n",
    "We now connect our newly created encoder with our decoder, to train an end-to-end seq2seq architecture. \n",
    "\n",
    "For the purposes of Part 1, the only interaction between the encoder and the decoder is that the *last hidden state of the encoder is used as the initial hidden state of the decoder*. This will be different for Part 2 and 3 where we will extend this punction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kN5nFk_ndkdU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(input_sentence, output_sentence, encoder,\n",
    "          decoder, encoder_optimizer,\n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          teacher_forcing_ratio = 1,\n",
    "          decoderType = \"LSTM\",\n",
    "          embeddings = one_hot_embeddings): \n",
    "    \"\"\"\n",
    "    Given a single training sample, go through a single step of training.\n",
    "    \"\"\"\n",
    "    use_teacher_forcing = True if np.random.rand() < teacher_forcing_ratio else False\n",
    "    # Your code goes here\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_cell = encoder.init_cell()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_tensor = torch.tensor(\n",
    "        [word2index.get(word, 0) for word in input_sentence],\n",
    "        dtype=torch.long\n",
    "    ).view(-1, 1)\n",
    "\n",
    "    input_sentence_one_hot_embeddings = embeddings[input_tensor]\n",
    "\n",
    "    output_tensor = torch.tensor(\n",
    "        [word2index.get(word, 0) for word in output_sentence],\n",
    "        dtype=torch.long\n",
    "    ).view(-1, 1)\n",
    "\n",
    "    output_sentence_one_hot_embeddings = embeddings[output_tensor]\n",
    "\n",
    "    input_length = input_sentence_one_hot_embeddings.shape[0]\n",
    "    output_length = output_sentence_one_hot_embeddings.shape[0]\n",
    "\n",
    "    # encoder_outputs = torch.zeros(maxSequenceLength, encoder.hidden_dim)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "      encoder_input = torch.tensor(input_sentence_one_hot_embeddings[ei,:], dtype=torch.float).view(1,1,-1)\n",
    "      encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    decoder_input = torch.tensor([word2index.get(\"<SOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "    decoder_input = embeddings[decoder_input]\n",
    "    decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "    EOS = torch.tensor([word2index.get(\"<EOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "    EOS = embeddings[EOS]\n",
    "    EOS = torch.tensor(EOS, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "    # print(decoder_input.shape)\n",
    "    # print(decoder_hidden.shape)\n",
    "    # print(decoder_cell.shape)\n",
    "\n",
    "    if decoderType == \"LSTM\": \n",
    "        if use_teacher_forcing: \n",
    "            # Your code goes here\n",
    "            for di in range(output_length):\n",
    "              try:\n",
    "                decoder_output, decoder_hidden, decoder_cell = decoder(\n",
    "                  decoder_input, decoder_hidden, decoder_cell\n",
    "                )\n",
    "              except:\n",
    "                print(decoder_input.shape)\n",
    "                print(decoder_hidden.shape)\n",
    "                print(decoder_cell.shape)\n",
    "                print(di)\n",
    "                print('use')\n",
    "                raise\n",
    "              output_tensor = output_sentence_one_hot_embeddings[di,:]\n",
    "              decoder_input = torch.tensor(output_tensor, dtype=torch.float).view(1,1,-1)\n",
    "              output_tensor = torch.tensor(output_tensor, dtype=torch.float).view(1,-1)\n",
    "              loss += criterion(decoder_output, output_tensor)\n",
    "            \n",
    "        elif not use_teacher_forcing:\n",
    "            # Your code goes here\n",
    "            for di in range(output_length):\n",
    "              try:\n",
    "                decoder_output, decoder_hidden, decoder_cell = decoder(\n",
    "                  decoder_input, decoder_hidden, decoder_cell\n",
    "                )\n",
    "              except:\n",
    "                print(decoder_input.shape)\n",
    "                print(decoder_hidden.shape)\n",
    "                print(decoder_cell.shape)\n",
    "                print('not use')\n",
    "                raise\n",
    "              topv, topi = decoder_output.topk(1)\n",
    "              decoder_input = topi.squeeze().detach()\n",
    "              decoder_input = embeddings[decoder_input]\n",
    "              decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "              output_tensor = output_sentence_one_hot_embeddings[di,:]\n",
    "              output_tensor = torch.tensor(output_tensor, dtype=torch.float).view(1,-1)\n",
    "              loss += criterion(decoder_output, output_tensor)\n",
    "              if decoder_input.item() == EOS:\n",
    "                break\n",
    "                \n",
    "    if decoderType == \"AttentionLSTM\": \n",
    "        dec_cell = decoder.init_cell(device=device).squeeze(1)\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "\n",
    "            out, _, dec_cell = decoder(input_sentence=tgt_code[:-1], hidden=out[-1:], cell=dec_cell, encoder_annotations=out)\n",
    "\n",
    "            final_loss = criterion(out, tgt_code[1:])\n",
    "\n",
    "        elif not use_teacher_forcing:\n",
    "            # Your code goes here\n",
    "            dec_hidden = out[-1:]\n",
    "            pred = []\n",
    "            out = tgt_code[:1]\n",
    "            for i in range(len(tgt_code)-1):\n",
    "                out, dec_hidden, dec_cell = decoder(input_sentence=out, hidden=dec_hidden, cell=dec_cell, encoder_annotations=out)\n",
    "                pred.append(out)\n",
    "            final_loss = criterion(torch.cat(pred), tgt_code[1:])\n",
    "\n",
    "    # if decoderType == \"Transformer\":\n",
    "        # Your code goes here (you can assume transformer uses teacher_forcing_ratio = 1)\n",
    "        # pred = decoder(input_sentence=output_tensor[:-1], hidden=encoder_hidden)\n",
    "        # final_loss=criterion(pred[0][0], output_tensor[1:])\n",
    "\n",
    "    # Your code goes here\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / output_length\n",
    "\n",
    "    # return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaEuQm7KwHBJ",
    "outputId": "118e5c75-6cae-4c16-dfc6-04541632ebe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training end-to-end network ......\n",
      "Single sentence Loss (epoch 0) : 2.987887\n",
      "Single sentence Loss (epoch 0) : 2.743317\n",
      "Single sentence Loss (epoch 0) : 3.103491\n",
      "Single sentence Loss (epoch 0) : 3.830954\n",
      "Single sentence Loss (epoch 0) : 3.364020\n",
      "Single sentence Loss (epoch 0) : 1.929029\n",
      "Single sentence Loss (epoch 0) : 3.930665\n",
      "Single sentence Loss (epoch 0) : 4.454616\n",
      "Single sentence Loss (epoch 0) : 4.206658\n",
      "Single sentence Loss (epoch 0) : 2.587494\n",
      "Single sentence Loss (epoch 0) : 2.915143\n",
      "Single sentence Loss (epoch 0) : 3.316535\n",
      "Single sentence Loss (epoch 0) : 4.125989\n",
      "Single sentence Loss (epoch 0) : 1.794005\n",
      "Single sentence Loss (epoch 0) : 4.354639\n",
      "Single sentence Loss (epoch 0) : 2.961681\n",
      "Single sentence Loss (epoch 0) : 3.303954\n",
      "Single sentence Loss (epoch 0) : 3.111633\n",
      "Single sentence Loss (epoch 0) : 2.536603\n",
      "Single sentence Loss (epoch 0) : 4.976700\n",
      "Single sentence Loss (epoch 0) : 3.044685\n",
      "Single sentence Loss (epoch 0) : 2.499129\n",
      "Single sentence Loss (epoch 0) : 2.849450\n",
      "Single sentence Loss (epoch 0) : 4.416542\n",
      "Single sentence Loss (epoch 0) : 4.082002\n",
      "Single sentence Loss (epoch 0) : 1.531735\n",
      "Single sentence Loss (epoch 0) : 3.066921\n",
      "Single sentence Loss (epoch 0) : 2.324676\n",
      "Single sentence Loss (epoch 0) : 1.892594\n",
      "Single sentence Loss (epoch 0) : 2.885733\n",
      "Single sentence Loss (epoch 0) : 1.552844\n",
      "Single sentence Loss (epoch 0) : 2.417839\n",
      "Single sentence Loss (epoch 0) : 2.279411\n",
      "Single sentence Loss (epoch 0) : 1.878100\n",
      "Single sentence Loss (epoch 0) : 2.458294\n",
      "Single sentence Loss (epoch 0) : 2.151941\n",
      "Single sentence Loss (epoch 0) : 1.120488\n",
      "Single sentence Loss (epoch 0) : 2.539536\n",
      "Single sentence Loss (epoch 0) : 2.495029\n",
      "Single sentence Loss (epoch 0) : 4.179136\n",
      "Loss (epoch 0) : 2.729211\n",
      "Single sentence Loss (epoch 1) : 1.887315\n",
      "Single sentence Loss (epoch 1) : 0.903477\n",
      "Single sentence Loss (epoch 1) : 2.202400\n",
      "Single sentence Loss (epoch 1) : 2.682927\n",
      "Single sentence Loss (epoch 1) : 1.913182\n",
      "Single sentence Loss (epoch 1) : 1.032790\n",
      "Single sentence Loss (epoch 1) : 2.768399\n",
      "Single sentence Loss (epoch 1) : 3.606547\n",
      "Single sentence Loss (epoch 1) : 2.855849\n"
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = DecoderLSTM()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end-to-end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, sentence in enumerate(filtered_sentences):\n",
    "        target_variable = piglatin_filtered_sentences[id]\n",
    "        # print(sentence)\n",
    "        # print(target_variable)\n",
    "        loss = train(sentence, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"LSTM\")\n",
    "\n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-KzySeXdkdP"
   },
   "source": [
    "## 1.4. Building Language Decoder MAP Inference\n",
    "\n",
    "We now define a method to perform inference with our decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fq1fkD-wdkdP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(sentence, encoder, decoder, decoderType=\"LSTM\", embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    input_tensor = torch.Tensor(preprocess_one_hot(sentence))\n",
    "    input_length = input_tensor.shape[0]\n",
    "    decoder_attentions = 0\n",
    "\n",
    "    # Initialize encoder & decoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_cell = encoder.init_cell()\n",
    "    decoder_hidden = decoder.init_hidden()\n",
    "    decoder_cell = decoder.init_hidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad() \n",
    "\n",
    "    for ei in range(1,input_length):\n",
    "        # Iteratively run the encoder \n",
    "        # 1. Get the current word index\n",
    "        # 2. Convert to a 1-hot encoding\n",
    "        encoder_input = input_tensor[ei].view(1,1,-1)\n",
    "        # 3. Run one step of the encoder\n",
    "        # 4. Save the encoder hidden states for future processing\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "\n",
    "    # Set the initial hidden and cell state of the RNN decoder to the last \n",
    "    # hidden and cell state of the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "\n",
    "    # Start the decoding with <SOS> token\n",
    "    decoder_input = torch.tensor([word2index.get(\"<SOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "    decoder_input = embeddings[decoder_input]\n",
    "    decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "    EOS = torch.tensor([word2index.get(\"<EOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    word_list = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # Iterate up to the max_length of output\n",
    "    for i in range(max_length):\n",
    "        if decoderType == \"LSTM\": \n",
    "            # Run the simple decoder\n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi == EOS:\n",
    "              word_list.append('<EOS>')\n",
    "              break\n",
    "            else:\n",
    "              word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "        if decoderType == \"AttentionLSTM\":\n",
    "            decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell, encoder_output\n",
    "            )\n",
    "            decoder_attentions[i] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi == EOS:\n",
    "              word_list.append('<EOS>')\n",
    "              break\n",
    "            else:\n",
    "              word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "        if decoderType == \"Transformer\":\n",
    "            decoder_output, encoder_attention_weights, self_attention_weights = decoder(decoder_input,encoder_output.unsqueeze())\n",
    "            topv, topi = decoder_output[0][-1].data.topk(1)\n",
    "            if topi.item() == EOS:\n",
    "              word_list.append('<EOS>')\n",
    "              break\n",
    "            else:\n",
    "              word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "    return word_list, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3E3scOrkxtHV"
   },
   "outputs": [],
   "source": [
    "# Lets test it \n",
    "sentence = \"A very clean and well decorated empty bathroom\" \n",
    "input_sentence = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] \n",
    "output_sentence, _ = inference(input_sentence, encoder, decoder, decoderType=\"LSTM\")\n",
    "\n",
    "print(\"English: \" + sentence)\n",
    "print(\"Pig Latin: \" + output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaZkwa6EdkdQ"
   },
   "source": [
    "## 1.5. Building Language Decoder Sampling Inference\n",
    "\n",
    "We now modify the inference method to sample from the distribution outputted by the LSTM rather than taking the most probable word.\n",
    "\n",
    "It might be useful to take a look at the output of your model and (depending on your implementation) modify it so that the outputs sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "chHsbrX8dkdQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sampling_inference(sentence, encoder, decoder, decoderType=\"LSTM\", embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    input_tensor = torch.Tensor(preprocess_one_hot(sentence))\n",
    "    input_length = input_tensor.shape[0]\n",
    "\n",
    "    # Initialize encoder & decoder \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_cell = encoder.init_cell()\n",
    "    decoder_hidden = decoder.init_hidden()\n",
    "    decoder_cell = decoder.init_hidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    for ei in range(1,input_length):\n",
    "        # Iteratively run the encoder \n",
    "        # 1. Get the current word index\n",
    "        # 2. Convert to a 1-hot encoding\n",
    "        encoder_input = input_tensor[ei].view(1,1,-1)\n",
    "        # 3. Run one step of the encoder\n",
    "        # 4. Save the encoder hidden states for future processing\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "\n",
    "    # Set the initial hidden and cell state of the RNN decoder to the last \n",
    "    # hidden and cell state of the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "\n",
    "    # Start the decoding with <SOS> token\n",
    "    decoder_input = torch.tensor([word2index.get(\"<SOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "    decoder_input = embeddings[decoder_input]\n",
    "    decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "    EOS = torch.tensor([word2index.get(\"<EOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    word_list = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # Iterate up to the max_length of output\n",
    "    for i in range(max_length):\n",
    "        if decoderType == \"LSTM\": \n",
    "            # Run the simple decoder \n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS:\n",
    "                word_list.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "        if decoderType == \"AttentionLSTM\":\n",
    "            decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell, encoder_output\n",
    "            )\n",
    "            decoder_attentions[i] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi == EOS:\n",
    "              word_list.append('<EOS>')\n",
    "              break\n",
    "            else:\n",
    "              word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "            \n",
    "\n",
    "        if decoderType == \"Transformer\":\n",
    "            decoder_output, encoder_attention_weights, self_attention_weights = decoder(decoder_input,encoder_output.unsqueeze())\n",
    "            topv, topi = decoder_output[0][-1].data.topk(1)\n",
    "            if topi.item() == EOS:\n",
    "              word_list.append('<EOS>')\n",
    "              break\n",
    "            else:\n",
    "              word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "\n",
    "    return word_list, decoder_attentions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6jeTIFOyfa7"
   },
   "outputs": [],
   "source": [
    "# Lets test it \n",
    "sentence = \"A very clean and well decorated empty bathroom\" \n",
    "input_sentence = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] \n",
    "\n",
    "print(\"English: \" + sentence)\n",
    "\n",
    "for i in range(5):\n",
    "    output_sentence, _ = sampling_inference(input_sentence, encoder, decoder, decoderType=\"LSTM\")\n",
    "    print(\"Pig Latin: \" + output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe3Aq_-VdkdV"
   },
   "source": [
    "## 1.6. Testing \n",
    "\n",
    "We must now define a method that allows us to do inference using the seq2seq architecture. We then run the 500 validation captions through this method, and ultimately compare the **reference** and **generated** sentences using our **BLEU** similarity score method defined above, to identify the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "rcxSh_RWdkdR",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, input_sentence in enumerate(val_sentences): \n",
    "    output_sentence= inference(sentence)\n",
    "    target_sentence = \"<SOS>\"+output_sentence\n",
    "    score = compute_bleu(\"<SOS>\"+target_sentence[0], \"<SOS>\"+output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 10 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLEU score : %f\" % (final_score)) \n",
    "\n",
    "\n",
    "# EXPECTED < Average BLUE score (ArgMAX inference): 0.464803 > \n",
    "# EXPECTED < Average BLUE score (sampling inference): 0.477803 > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tL3LKIZ7dkdQ"
   },
   "source": [
    "## 1.7. Experiment with Teacher Forcing\n",
    "\n",
    "Redo steps 1.3 and 1.6 with teacher_forcing_ratio = 0.9 and 0.8. Comment on the results, speed of convergence and the quality of results. Note that in most real scenarious the teacher forcing is actually annealed; starting with teacher forcing = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DX-D_PI7dkdV",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "decoder = DecoderLSTM(wordEncodingSize,vocabularySize)\n",
    "teacher_forcing_ratio = 0.9\n",
    "train(decoder)\n",
    " \n",
    "\n",
    "decoder = DecoderLSTM(wordEncodingSize,vocabularySize)\n",
    "teacher_forcing_ratio = 0.8\n",
    "train(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3wDN2Z1dkdX"
   },
   "source": [
    "## 1.8. Encoding as Generic Feature Representation\n",
    "\n",
    "We now use the final hidden state of our encoder, to identify the nearest neighbor amongst the training sentences for each sentence in our validation data.\n",
    "\n",
    "It would be effective to first define a method that would generate all of the hidden states and store these hidden states **on the CPU**, and then loop over the generated hidden states to identify/output the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IxFmDA_YdkdY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from os import device_encoding\n",
    "\n",
    "def final_encoder_hidden(sentence, embeddings):\n",
    "    # Your code goes here\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    numerized = preprocess_numberize(sentence)\n",
    "    sentence_one_hot_embeddings = embeddings[numerized]\n",
    "    input_length = sentence_one_hot_embeddings.shape[0]\n",
    "    for ei in range(input_length):\n",
    "        encoder_input = torch.tensor(sentence_one_hot_embeddings[ei,:], dtype=torch.float, \n",
    "                                 device=device_encoding).view(1,1,-1)\n",
    "        encoder_output, encoder_hidden = encoder(encoder_input , encoder_hidden)\n",
    "        \n",
    "    return encoder_output.view(-1)\n",
    "# Now run all training data and validation data to store hidden states\n",
    "    # Your code goes here\n",
    "val_contexts = []\n",
    "train_contexts = []\n",
    "for sentence in val_sentences:\n",
    "    val_contexts.append(final_encoder_hidden(sentence).cpu().data.numpy())\n",
    "np.save(open('validation_vectors', 'wb+'), val_contexts)\n",
    "    \n",
    "for sentence in train_sentences:\n",
    "    train_contexts.append(final_encoder_hidden(sentence).cpu().data.numpy())\n",
    "np.save(open('training_vectors', 'wb+'), train_contexts)\n",
    "\n",
    "val_contexts = np.load('validation_vectors')\n",
    "train_contexts = np.load('training_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RpZtVzHpdkdY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Now get nearest neighbors and print\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distances = euclidean_distances(val_contexts, train_contexts)\n",
    "min_distances = np.argmin(distances, axis=1)\n",
    "print(min_distances.shape)\n",
    "print(min_distances.shape)\n",
    "i = 0\n",
    "for index in min_distances[:10]:\n",
    "    print(val_sentences[i])\n",
    "    i = i+1\n",
    "    print(train_sentences[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFpfb1oi0efe"
   },
   "source": [
    "# Part 2: Attention LSTM Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx2bELJh03hq"
   },
   "source": [
    "## 2.1. Implementing Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2adPSOL09Hj"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_network = nn.Sequential(\n",
    "                                    nn.Linear(hidden_size*2, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, 1)\n",
    "                                 )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the additive attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x 1 x seq_len)\n",
    "\n",
    "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # Your code goes here\n",
    "        # ------------\n",
    "        batch_size, seq_len, hidden_size = keys.size()\n",
    "        expanded_queries = queries.unsqueeze(1).expand_as(keys)\n",
    "        concat_inputs = torch.cat((expanded_queries, keys), 2)\n",
    "        unnormalized_attention = self.attention_network(concat_inputs.view(-1, 2 * hidden_size)).view(batch_size, seq_len, 1)\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), values)\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOPm_dvX1TrZ"
   },
   "source": [
    "## 2.2. Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY7vHNT_1oug"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        self.dropout_p = 0.1\n",
    "        self.linear_input = nn.Linear(wordEncodingSize, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.hidden_dim*2, self.hidden_dim)\n",
    "        self.attention = AdditiveAttention(hidden_size=self.hidden_dim)\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.cell = self.init_cell()\n",
    "\n",
    "    def init_hidden(self):    \n",
    "        return torch.randn(1,1, self.hidden_dim).cuda()\n",
    "\n",
    "    def init_cell(self):\n",
    "        return torch.randn(1,1, self.hidden_dim).cuda()\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell, encoder_annotations):\n",
    "        embed = self.dropout(self.linear_input(input_sentence.view(1,-1)))\n",
    "\n",
    "        # ------------\n",
    "        # Your code goes here\n",
    "        # ------------\n",
    "        batch_size, seq_len = input_sentence.size()\n",
    "        hidden = []\n",
    "        attention = []\n",
    "        prev = hidden\n",
    "        for i in range (seq_len):\n",
    "            embed_current = embed[:, i, :]\n",
    "            context, attention_weights = self.attention(embed_current, cell, encoder_annotations)\n",
    "            embed_and_context = torch.cat((embed_current, context.squeeze(1)), 1)\n",
    "            prev = self.lstm(embed_and_context, prev)\n",
    "            \n",
    "            hidden.append(prev)\n",
    "            attention.append(attention_weights)\n",
    "        \n",
    "        hidden = torch.stack(hidden, dim=1)\n",
    "        attention = torch.cat(attention, dim=2)\n",
    "        output = self.linear(hidden)\n",
    "        \n",
    "        return output, hidden, cell, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db7mTRflmQQu"
   },
   "source": [
    "## 2.3. Training Attention Decoder\n",
    "\n",
    "Note that you will need to modify the train() procedure for Part 1 to handles the AttentionLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEYM3U0VmdHp"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = AttentionDecoder()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end to end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, sentence in enumerate(filtered_sentences):\n",
    "        target_variable = piglatin_filtered_sentences[id]\n",
    "        loss = train(sentence, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"AttentionLSTM\")\n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFTuFon2m7f9"
   },
   "source": [
    "## 2.4. Testing Attention Decoder\n",
    "Note that you will need to modify the inference() procedure for Part 1 to handle Attention LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx3flyVdnGCg"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, input_sentence in enumerate(val_sentences): \n",
    "    output_sentence= inference(sentence)\n",
    "    target_sentence = \"<SOS>\"+output_sentence\n",
    "    score = compute_bleu(\"<SOS>\"+target_sentence[0], \"<SOS>\"+output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 10 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLEU score : %f\" % (final_score))  \n",
    "\n",
    "# EXPECTED < Average BLUE score (ArgMAX inference): 0.739589 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvKl0zhhngVg"
   },
   "source": [
    "## 2.5. Visualize Attention for Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsAfscbVnhF6"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +['<EOS>'])\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "score1 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[0])\n",
    "score2 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[5])\n",
    "\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[0] + '\" \\nis: ' + str(score1) +'\\n\\n')\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[5] + '\" \\nis: ' + str(score2) +'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XABkHOBJns76"
   },
   "source": [
    "# Part 3: Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFb5tCzOpDmB"
   },
   "source": [
    "## 3.1 Implement Scaled Dot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDgSOIvdpCuR"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # Your code goes here\n",
    "        # ------------\n",
    "        batch_size = queries.size(0)\n",
    "        if queries.dim() != 3:\n",
    "            queries = torch.unsqueeze(queries, dim = 1)\n",
    "        q = self.Q(queries)\n",
    "        k = self.K(keys)\n",
    "        k = torch.transpose(k, 1, 2)\n",
    "        v = self.V(values)\n",
    "        unnormalized_attention = torch.bmm(q,k) * self.scaling_factor\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights, v)\n",
    "\n",
    "        return context, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4gpSwDopbHZ"
   },
   "source": [
    "## 3.2. Implement Causal Scaled Dot Attention\n",
    "\n",
    "The implementation should be nearly identical to the one above, but with mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFnGpHklpbpQ"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "class CausalScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.neg_inf = torch.tensor(-1e7).cuda()\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        NOTES:\n",
    "            batch_size = 1\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "                In training k = maxSequenceLength or length of the GT ourput sequence\n",
    "                In testing k = length of currently decoded sub-sequence\n",
    "            keys: The decoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The decoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # Your code goes here\n",
    "        # ------------\n",
    "        batch_size = queries.size(0)\n",
    "\n",
    "        k_value = queries.size(1)\n",
    "        seq_len = keys.size(1)\n",
    "\n",
    "        if queries.dim() != 3:\n",
    "            queries = torch.unsqueeze(queries, dim = 1)\n",
    "        q = self.Q(queries)\n",
    "        k = self.K(keys)\n",
    "        k = torch.transpose(k, 1, 2)\n",
    "        v = self.V(values)\n",
    "        unnormalized_attention = torch.bmm(q,k) * self.scaling_factor\n",
    "        mask = torch.tril(torch.ones(batch_size, k_value, seq_len))\n",
    "        unnormalized_attention[mask == 0] = self.neg_inf\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights, v)\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsLrprNLqQ5x"
   },
   "source": [
    "## 3.3. Implement Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjiwfUHXqRXI"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        self.dropout_p = 0.1\n",
    "        self.num_layers = 3\n",
    "        self.linear_input = nn.Linear(wordEncodingSize, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
    "                                    hidden_size=self.hidden_dim, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=self.hidden_dim, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                                    nn.ReLU(),\n",
    "                                 ) for i in range(self.num_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell, annotations):\n",
    "        embed = self.dropout(self.linear_input(input_sentence)).unsqueeze(0)\n",
    "        \n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        batch_size, seq_len, hidden_size = contexts.size()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # ------------\n",
    "            new_contexts, self_attention_weights = self.self_attentions[i](contexts,contexts,contexts)\n",
    "            residual_contexts = contexts + new_contexts\n",
    "\n",
    "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts,annotations,annotations)\n",
    "            residual_contexts = residual_contexts + new_contexts\n",
    "            new_contexts = self.attention_mlps[i](residual_contexts.view(-1, self.hidden_size)).view(batch_size, seq_len, self.hidden_size)\n",
    "            contexts = residual_contexts + new_contexts\n",
    "            # ------------\n",
    "            \n",
    "            \n",
    "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "            self_attention_weights_list.append(self_attention_weights)            \n",
    "        \n",
    "        output = self.linear(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "        \n",
    "        return output, encoder_attention_weights, self_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENolyKTWrAWq"
   },
   "source": [
    "## 3.4. Training Transformer Decoder\n",
    "\n",
    "Note that you will need to modify the train() procedure for Part 1 to handle the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iuUnzkZrA44"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = TransformerDecoder()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end to end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, sentence in enumerate(filtered_sentences):\n",
    "        target_variable = piglatin_filtered_sentences[id]\n",
    "        loss = train(sentence, target_variable, encoder, decoder,encoder_optimizer,decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"Transformer\")\n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOF6M3tWrbOS"
   },
   "source": [
    "## 3.5. Testing Transformer Decoder\n",
    "Note that you will need to modify the inference() procedure for Part 1 to handle Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2BlDGfcroOA"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, input_sentence in enumerate(val_sentences): \n",
    "    output_sentence= inference(sentence)\n",
    "    target_sentence = \"<SOS>\"+output_sentence\n",
    "    score = compute_bleu(\"<SOS>\"+target_sentence[0], \"<SOS>\"+output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 10 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLEU score : %f\" % (final_score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCrOGW2NrzPG"
   },
   "source": [
    "## 3.6 Visualizing Attention for Transformer Decoder\n",
    "\n",
    "Note that since we have multiple attention layers, there will be one attention to be visualized per layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +['<EOS>'])\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "score1 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[0])\n",
    "score2 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[5])\n",
    "\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[0] + '\" \\nis: ' + str(score1) +'\\n\\n')\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[5] + '\" \\nis: ' + str(score2) +'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jng5Ksadkdp"
   },
   "source": [
    "# 4. Effectiveness of word2vec\n",
    "\n",
    "As an option, you may repeat one of the models above by modifying the code to use word2vec embedding for the input English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RWVVvgyudkdq",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
