{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnzrOsd0bZHS"
   },
   "source": [
    "# CPSC532S Assignment 3:  RNNs for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "t_cnRUY7dkdI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oSmx8s7dkdK",
    "tags": []
   },
   "source": [
    "# Data Acquisition\n",
    "\n",
    "\n",
    "The goal of this assignment is to translate English to Pig Latin. For this assignment, you must download the data and extract it into `data/`. The dataset contains four files, each containing a single caption on each line. There are two files for training (English vs Pig Latin) and two files for validation. We should have 20,000 sentences (one sentence per image in Assignment 2) in the training captions and 500 sentences in the validation captions (five sentences per image in Assignment 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS3DYtiqdkdL",
    "outputId": "b7c2ef71-f89a-401b-b9c2-670dfc0fd92e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "500\n",
      "5000\n",
      "500\n",
      "A very clean and well decorated empty bathroom\n",
      "Away eryvay eanclay andway ellway ecoratedday emptyway athroombay\n",
      "Set of bananas hanging off of a banana tree.\n",
      "Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/data')\n",
    "\n",
    "# Load the data into memory.\n",
    "mscoco_train = json.load(open(\"./content/train_captions.json\"))\n",
    "mscoco_val  = json.load(open(\"./content/val_captions.json\"))\n",
    "\n",
    "mscoco_piglatin_train = json.load(open('./content/piglatin_train_captions.json'))\n",
    "mscoco_piglatin_val  = json.load(open('./content/piglatin_val_captions.json'))\n",
    "\n",
    "train_sentences = [entry['caption'] for entry in mscoco_train['annotations']]\n",
    "train_sentences = train_sentences[:5000]\n",
    "val_sentences = [entry['caption'] for entry in mscoco_val['annotations']]\n",
    "\n",
    "piglatin_train_sentences = [entry['caption'] for entry in mscoco_piglatin_train['annotations']]\n",
    "piglatin_train_sentences = piglatin_train_sentences[:5000]\n",
    "piglatin_val_sentences = [entry['caption'] for entry in mscoco_piglatin_val['annotations']]\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(piglatin_train_sentences))\n",
    "print(len(piglatin_val_sentences))\n",
    "print(train_sentences[0])\n",
    "print(piglatin_train_sentences[0])\n",
    "print(val_sentences[0])\n",
    "print(piglatin_val_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eH3KLA_NIqZR",
    "outputId": "632775b9-cf14-470b-d781-65fd5df5e1ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xuanchen/Desktop/532S/assignment/Assignment3\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyGm8VmCdkdM"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "The code provided below creates word embeddings for you to use. After creating the vocabulary, we construct both one-hot embeddings and word2vec embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYWipM4tdkdN",
    "outputId": "c9e42e5d-8fd8-4fcc-8cf2-ff1671a45e7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xuanchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = train_sentences\n",
    "piglatin_sentences = piglatin_train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "piglatin_sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in piglatin_sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 2000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "piglatin_word_counts = Counter([word for sentence in piglatin_sentences for word in sentence])\n",
    "word_counts = word_counts + piglatin_word_counts\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "\n",
    "# Build the one hot embeddings\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "piglatin_filtered_sentences = [[word for word in sentence if word in word2index] for sentence in piglatin_sentences]\n",
    "all_filtered_sentences = filtered_sentences + piglatin_filtered_sentences\n",
    "w2v = Word2Vec(all_filtered_sentences, min_count=0, vector_size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.vectors))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "piglatin_maxSequenceLength = max([len(sentence) for sentence in piglatin_sentences])\n",
    "\n",
    "if piglatin_maxSequenceLength > maxSequenceLength:\n",
    "    maxSequenceLength = piglatin_maxSequenceLength\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zYnaPC0i0A2"
   },
   "source": [
    "# Utilities functions\n",
    "\n",
    "\n",
    "Please look through the functions provided below carefully, as you will need to use all of them at some point in your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PFOm2o3jINX",
    "outputId": "06151d19-916e-486e-82b3-7cee77ce31cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distnace between \n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "and\n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "is: 1.0\n",
      "\n",
      "\n",
      "BLEU score distnace between \n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "and\n",
      "  \"A few people sit on a dim transportation system. \" \n",
      "is: 0.1933853138176172\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentence, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a reference sentence, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = word_tokenize(reference_sentence.lower())\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu([reference_tokenized], predicted_tokenized)\n",
    "\n",
    "%matplotlib inline\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +['<EOS>'])\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "score1 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[0])\n",
    "score2 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[5])\n",
    "\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[0] + '\" \\nis: ' + str(score1) +'\\n\\n')\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[5] + '\" \\nis: ' + str(score2) +'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZD-r_MckSLA"
   },
   "source": [
    "#Part 1: Encoder-Decoder Language Translation with Teacher-Forcing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSOKOJModkdN"
   },
   "source": [
    "## 1.1 Building a Language Decoder\n",
    "\n",
    "We now implement a language decoder. For now, we will have the decoder take a single training sample at a time (as opposed to batching). For our purposes, we will also avoid defining the embeddings as part of the model and instead pass in embedded inputs. While this is sometimes useful, as it learns/tunes the embeddings, we avoid doing it for the sake of simplicity and speed.\n",
    "\n",
    "Remember to use LSTM hidden units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "w7g1DsQcdkdO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        \n",
    "        # Your code goes here (~4 lines or less)\n",
    "        self.lstm = nn.LSTM(wordEncodingSize, self.hidden_dim)\n",
    "        self.linear = nn.Linear(self.hidden_dim, wordEncodingSize)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "\n",
    "\n",
    "    def init_cell(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(input_sentence, (hidden, cell))\n",
    "        output = self.softmax(self.linear(output[0]))\n",
    "        return output, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJVTRdUkdkdR"
   },
   "source": [
    "## 1.2.  Building Language Encoder\n",
    "\n",
    "We now build a language encoder, which will encode an input word by word, and ultimately output a hidden state that we can then be used by our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "kccifunUdkdR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        \n",
    "        # Your code goes here (~3 lines)\n",
    "        self.lstm = nn.LSTM(input_size=wordEncodingSize, hidden_size=self.hidden_dim)\n",
    "       \n",
    "    def init_hidden(self):\n",
    "        # Your code goes here (1 line)\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "    \n",
    "    def init_cell(self):\n",
    "        # Your code goes here (1 line)\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell):\n",
    "        # Your code goes here (~2 lines of code)\n",
    "        output, (hidden, cell) = self.lstm(input_sentence, (hidden, cell))\n",
    "        \n",
    "        return output, hidden, cell\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9i-rNQ8sGdrN",
    "outputId": "842fe8d3-521a-49be-a35e-b4bf663c1593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 2000)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor(\n",
    "        preprocess_numberize(\"A very clean and well decorated empty bathroom\"),\n",
    "        dtype=torch.long\n",
    "    ).view(-1, 1)\n",
    "encoder = EncoderLSTM()\n",
    "encoder_hidden = encoder.init_hidden()\n",
    "encoder_cell = encoder.init_cell()\n",
    "input_sentence_one_hot_embeddings = one_hot_embeddings[input_tensor]\n",
    "print(input_sentence_one_hot_embeddings.shape)\n",
    "input_length = input_sentence_one_hot_embeddings.shape[0]\n",
    "\n",
    "# for ei in range(input_length):\n",
    "#   encoder_input = torch.tensor(input_sentence_one_hot_embeddings[ei,:], dtype=torch.float).view(1,1,-1)\n",
    "#   encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "# print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQDOBB03UhIP",
    "outputId": "9064aafa-2889-4f9b-d335-a8021d228a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 300])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "test = torch.zeros(maxSequenceLength, 300)\n",
    "print(test.shape)\n",
    "decoder_input = torch.tensor([preprocess_numberize(\"<SOS>\")])\n",
    "print(decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7y9AKd5ePCV",
    "outputId": "a5e521d5-6455-44e5-eafc-be2e07fef43f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2000])\n",
      "tensor([[85]])\n",
      "torch.Size([1, 1, 2000])\n",
      "tensor([[2]])\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderLSTM()\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder_cell = encoder_cell\n",
    "decoder_input = torch.tensor([word2index.get(\"<SOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "decoder_input = one_hot_embeddings[decoder_input]\n",
    "decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "for di in range(input_length):\n",
    "  decoder_output, decoder_hidden, decoder_cell = decoder(\n",
    "      decoder_input, decoder_hidden, decoder_cell\n",
    "  )\n",
    "print(decoder_output.shape)\n",
    "topv, topi = decoder_output.topk(1)\n",
    "decoder_input = topi.squeeze().detach()\n",
    "decoder_input = one_hot_embeddings[decoder_input]\n",
    "decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "print(topi)\n",
    "print(decoder_input.shape)\n",
    "test = torch.tensor([word2index.get(\"<EOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljBX3m0tdkdT"
   },
   "source": [
    "## 1.3. Connecting Encoder to Decoder and Train End-to-End and Train with Teacher Forcing\n",
    "\n",
    "We now connect our newly created encoder with our decoder, to train an end-to-end seq2seq architecture. \n",
    "\n",
    "For the purposes of Part 1, the only interaction between the encoder and the decoder is that the *last hidden state of the encoder is used as the initial hidden state of the decoder*. This will be different for Part 2 and 3 where we will extend this punction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "kN5nFk_ndkdU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import device_encoding\n",
    "\n",
    "\n",
    "def train(input_sentence, output_sentence, encoder,\n",
    "          decoder, encoder_optimizer,\n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          teacher_forcing_ratio = 1,\n",
    "          decoderType = \"LSTM\",\n",
    "          embeddings = one_hot_embeddings): \n",
    "    \"\"\"\n",
    "    Given a single training sample, go through a single step of training.\n",
    "    \"\"\"\n",
    "    use_teacher_forcing = True if np.random.rand() < teacher_forcing_ratio else False\n",
    "    # Your code goes here\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_cell = encoder.init_cell()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_tensor = torch.tensor(\n",
    "        [word2index.get(word, 0) for word in input_sentence],\n",
    "        dtype=torch.long\n",
    "    ).view(-1, 1)\n",
    "\n",
    "    input_sentence_one_hot_embeddings = embeddings[input_tensor]\n",
    "\n",
    "    output_tensor = torch.tensor(\n",
    "        [word2index.get(word, 0) for word in output_sentence],\n",
    "        dtype=torch.long\n",
    "    ).view(-1, 1)\n",
    "\n",
    "    output_sentence_one_hot_embeddings = embeddings[output_tensor]\n",
    "\n",
    "    input_length = input_sentence_one_hot_embeddings.shape[0]\n",
    "    output_length = output_sentence_one_hot_embeddings.shape[0]\n",
    "    \n",
    "    encoder_hidden_states = torch.zeros(maxSequenceLength, encoder.hidden_dim)\n",
    "\n",
    "    # encoder_outputs = torch.zeros(maxSequenceLength, encoder.hidden_dim)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_input = torch.tensor(input_sentence_one_hot_embeddings[ei,:], dtype=torch.float).view(1,1,-1)\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "        encoder_hidden_states[ei,:] = encoder_hidden[0]\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    decoder_input = torch.tensor([word2index.get(\"<SOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "    decoder_input = embeddings[decoder_input]\n",
    "    decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "    EOS = torch.tensor([word2index.get(\"<EOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "    EOS = embeddings[EOS]\n",
    "    EOS = torch.tensor(EOS, dtype=torch.float).view(1,1,-1)\n",
    "    \n",
    "    output_len = len(output_sentence)\n",
    "    if decoderType == \"Transformer\":\n",
    "        full_sentence_decoder_input = torch.zeros(output_len, vocabularySize)\n",
    "        full_sentence_decoder_input[0,:] = decoder_input.unsqueeze(0)\n",
    "        full_sentence_target_output = torch.zeros(output_len, dtype=torch.int64)\n",
    "\n",
    "\n",
    "    # print(decoder_input.shape)\n",
    "    # print(decoder_hidden.shape)\n",
    "    # print(decoder_cell.shape)\n",
    "\n",
    "    if decoderType == \"LSTM\": \n",
    "        if use_teacher_forcing: \n",
    "            # Your code goes here\n",
    "            for di in range(output_length):\n",
    "                try:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = decoder(\n",
    "                        decoder_input, decoder_hidden, decoder_cell\n",
    "                    )\n",
    "                except:\n",
    "                    print(decoder_input.shape)\n",
    "                    print(decoder_hidden.shape)\n",
    "                    print(decoder_cell.shape)\n",
    "                    print(di)\n",
    "                    print('use')\n",
    "                    raise\n",
    "                output_tensor = output_sentence_one_hot_embeddings[di,:]\n",
    "                decoder_input = torch.tensor(output_tensor, dtype=torch.float).view(1,1,-1)\n",
    "                output_tensor = torch.tensor(output_tensor, dtype=torch.float).view(1,-1)\n",
    "                loss += criterion(decoder_output, output_tensor)\n",
    "            \n",
    "        elif not use_teacher_forcing:\n",
    "            # Your code goes here\n",
    "            for di in range(output_length):\n",
    "                try:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = decoder(\n",
    "                        decoder_input, decoder_hidden, decoder_cell\n",
    "                    )\n",
    "                except:\n",
    "                    print(decoder_input.shape)\n",
    "                    print(decoder_hidden.shape)\n",
    "                    print(decoder_cell.shape)\n",
    "                    print('not use')\n",
    "                    raise\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = embeddings[decoder_input]\n",
    "                decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "                output_tensor = output_sentence_one_hot_embeddings[di,:]\n",
    "                output_tensor = torch.tensor(output_tensor, dtype=torch.float).view(1,-1)\n",
    "                loss += criterion(decoder_output, output_tensor)\n",
    "                if topi.squeeze().item() == word2index[\"<EOS>\"]:\n",
    "                      break\n",
    "                # if decoder_input.item() == EOS:\n",
    "                #       break\n",
    "                \n",
    "    if decoderType == \"AttentionLSTM\":\n",
    "        for i in range(output_length):\n",
    "            decoder_output, decoder_hidden, decoder_cell, attention_weights = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_hidden_states)\n",
    "            next_word = output_sentence[i]\n",
    "            next_word_indx = word2index[next_word]\n",
    "            decoder_word_input = embeddings[next_word_indx]\n",
    "            target_output = torch.LongTensor(np.nonzero(decoder_word_input)[0])\n",
    "            \n",
    "            if use_teacher_forcing: \n",
    "                decoder_input = torch.FloatTensor(decoder_word_input)\n",
    "            elif not use_teacher_forcing:\n",
    "                topv, topi = log_softmax(decoder_output[0]).topk(1)\n",
    "                decoder_input = torch.FloatTensor(embeddings[topi.squeeze().detach().item()])\n",
    "            \n",
    "            if decoderType != \"Transformer\":     \n",
    "                loss += criterion(decoder_output[0], target_output)\n",
    "            \n",
    "            if not use_teacher_forcing:\n",
    "                if topi.squeeze().item() == word2index[\"<EOS>\"]:\n",
    "                    break\n",
    "\n",
    "    if decoderType == \"Transformer\":\n",
    "        for i in range(1, output_length):\n",
    "            next_word = output_sentence[i]\n",
    "            next_word_indx = word2index[next_word]\n",
    "            decoder_word_input = embeddings[next_word_indx]\n",
    "            target_output = torch.LongTensor(np.nonzero(decoder_word_input)[0])\n",
    "\n",
    "            # Your code goes here (you can assume transformer uses teacher_forcing_ratio = 1)\n",
    "            full_sentence_decoder_input[i,:] = torch.FloatTensor(decoder_word_input)\n",
    "            full_sentence_target_output[i-1] = torch.LongTensor(np.nonzero(decoder_word_input)[0])\n",
    "         \n",
    "\n",
    "    if decoderType == \"Transformer\":\n",
    "        full_sentence_decoder_output, self_attn, encoder_decoder_attn = decoder(full_sentence_decoder_input, decoder_hidden, decoder_cell, encoder_hidden_states)\n",
    "        loss += criterion(full_sentence_decoder_output[0], full_sentence_target_output) \n",
    "\n",
    "    # Your code goes here\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaEuQm7KwHBJ",
    "outputId": "118e5c75-6cae-4c16-dfc6-04541632ebe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training end-to-end network ......\n",
      "Single sentence Loss (epoch 0) : 4.028410\n",
      "Single sentence Loss (epoch 0) : 2.726545\n",
      "Single sentence Loss (epoch 0) : 3.118297\n",
      "Single sentence Loss (epoch 0) : 3.166363\n",
      "Single sentence Loss (epoch 0) : 3.310811\n",
      "Single sentence Loss (epoch 0) : 2.493892\n",
      "Single sentence Loss (epoch 0) : 4.538890\n",
      "Single sentence Loss (epoch 0) : 4.550961\n",
      "Single sentence Loss (epoch 0) : 4.187765\n",
      "Single sentence Loss (epoch 0) : 2.731255\n",
      "Loss (epoch 0) : 3.386638\n",
      "Single sentence Loss (epoch 1) : 3.213170\n",
      "Single sentence Loss (epoch 1) : 1.801395\n",
      "Single sentence Loss (epoch 1) : 2.900025\n",
      "Single sentence Loss (epoch 1) : 2.930545\n",
      "Single sentence Loss (epoch 1) : 2.381593\n",
      "Single sentence Loss (epoch 1) : 2.086346\n",
      "Single sentence Loss (epoch 1) : 3.730067\n",
      "Single sentence Loss (epoch 1) : 3.884600\n",
      "Single sentence Loss (epoch 1) : 3.335449\n",
      "Single sentence Loss (epoch 1) : 2.443882\n",
      "Loss (epoch 1) : 2.761695\n",
      "Single sentence Loss (epoch 2) : 2.939102\n",
      "Single sentence Loss (epoch 2) : 1.430771\n",
      "Single sentence Loss (epoch 2) : 2.722603\n",
      "Single sentence Loss (epoch 2) : 2.600858\n",
      "Single sentence Loss (epoch 2) : 2.026767\n",
      "Single sentence Loss (epoch 2) : 1.820204\n",
      "Single sentence Loss (epoch 2) : 3.369223\n",
      "Single sentence Loss (epoch 2) : 3.503451\n",
      "Single sentence Loss (epoch 2) : 3.118404\n",
      "Single sentence Loss (epoch 2) : 2.125329\n",
      "Loss (epoch 2) : 2.449675\n",
      "Single sentence Loss (epoch 3) : 2.782456\n",
      "Single sentence Loss (epoch 3) : 1.166622\n",
      "Single sentence Loss (epoch 3) : 2.410469\n",
      "Single sentence Loss (epoch 3) : 2.231629\n",
      "Single sentence Loss (epoch 3) : 1.751196\n",
      "Single sentence Loss (epoch 3) : 1.559070\n",
      "Single sentence Loss (epoch 3) : 3.209337\n",
      "Single sentence Loss (epoch 3) : 3.177050\n",
      "Single sentence Loss (epoch 3) : 2.884724\n",
      "Single sentence Loss (epoch 3) : 1.720698\n",
      "Loss (epoch 3) : 2.167260\n",
      "Single sentence Loss (epoch 4) : 2.387904\n",
      "Single sentence Loss (epoch 4) : 0.927836\n",
      "Single sentence Loss (epoch 4) : 2.127489\n",
      "Single sentence Loss (epoch 4) : 1.909831\n",
      "Single sentence Loss (epoch 4) : 1.438333\n",
      "Single sentence Loss (epoch 4) : 1.214524\n",
      "Single sentence Loss (epoch 4) : 2.844908\n",
      "Single sentence Loss (epoch 4) : 2.719103\n",
      "Single sentence Loss (epoch 4) : 2.617226\n",
      "Single sentence Loss (epoch 4) : 1.396894\n",
      "Loss (epoch 4) : 1.884566\n"
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = DecoderLSTM()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end-to-end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, sentence in enumerate(filtered_sentences):\n",
    "        target_variable = piglatin_filtered_sentences[id]\n",
    "        # print(sentence)\n",
    "        # print(target_variable)\n",
    "        loss = train(sentence, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"LSTM\")\n",
    "\n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-KzySeXdkdP"
   },
   "source": [
    "## 1.4. Building Language Decoder MAP Inference\n",
    "\n",
    "We now define a method to perform inference with our decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "Fq1fkD-wdkdP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(sentence, encoder, decoder, decoderType=\"LSTM\", embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    input_length = len(sentence)\n",
    "    decoder_attentions = 0\n",
    "\n",
    "    # Initialize encoder & decoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_cell = encoder.init_cell()\n",
    "    # decoder_hidden = decoder.init_hidden()\n",
    "    # decoder_cell = decoder.init_hidden()\n",
    "    encoder_hidden_states = torch.zeros(max_length, encoder.hidden_dim)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad() \n",
    "\n",
    "    for ei in range(1,input_length):\n",
    "        # Iteratively run the encoder \n",
    "        # 1. Get the current word index\n",
    "        wordIdx = word2index[sentence[ei]]\n",
    "        # 2. Convert to a 1-hot encoding\n",
    "        encoder_input = torch.Tensor(embeddings[wordIdx]).view(1,1,-1)\n",
    "        # 3. Run one step of the encoder\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "        # 4. Save the encoder hidden states for future processing\n",
    "        encoder_hidden_states[ei,:] = encoder_hidden[0]\n",
    "\n",
    "    # Set the initial hidden and cell state of the RNN decoder to the last \n",
    "    # hidden and cell state of the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "\n",
    "    # Start the decoding with <SOS> token\n",
    "    decoder_input = torch.tensor([word2index.get(\"<SOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "    decoder_input = embeddings[decoder_input]\n",
    "    decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "    EOS = torch.tensor([word2index.get(\"<EOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    word_list = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # Iterate up to the max_length of output\n",
    "    for i in range(max_length):\n",
    "        if decoderType == \"LSTM\": \n",
    "            # Run the simple decoder\n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi == EOS:\n",
    "                word_list.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "        if decoderType == \"AttentionLSTM\":\n",
    "            decoder_output, decoder_hidden, decoder_cell, attention_weights = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_hidden_states)\n",
    "            decoder_attentions[i] = attention_weights.detach().cpu()[0,:,0]\n",
    "            \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi == EOS:\n",
    "                word_list.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "        if decoderType == \"Transformer\":\n",
    "            decoder_output, encoder_decoder_attn, _ = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_hidden_states)\n",
    "            decoder_attentions = encoder_decoder_attn\n",
    "            decoder_output = decoder_output[:,-1:,:]\n",
    "            topv, topi = decoder_output[0][-1].data.topk(1)\n",
    "            if topi.item() == EOS:\n",
    "                word_list.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "    \n",
    "    res_list = \"\"\n",
    "    for i in range(len(word_list)):\n",
    "        if word_list[i] == \"<EOS>\" or word_list[i] == \"<SOS>\":\n",
    "            continue\n",
    "        res_list = res_list + \" \" + word_list[i]\n",
    "\n",
    "    return res_list, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "3E3scOrkxtHV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: A very clean and well decorated empty bathroom\n",
      "Pig Latin:  away ofway itewhay andway away argelay aintray .\n"
     ]
    }
   ],
   "source": [
    "# Lets test it \n",
    "sentence = \"A very clean and well decorated empty bathroom\" \n",
    "input_sentence = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] \n",
    "output_sentence, _ = inference(input_sentence, encoder, decoder, decoderType=\"LSTM\")\n",
    "\n",
    "print(\"English: \" + sentence)\n",
    "print(\"Pig Latin: \" + output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaZkwa6EdkdQ"
   },
   "source": [
    "## 1.5. Building Language Decoder Sampling Inference\n",
    "\n",
    "We now modify the inference method to sample from the distribution outputted by the LSTM rather than taking the most probable word.\n",
    "\n",
    "It might be useful to take a look at the output of your model and (depending on your implementation) modify it so that the outputs sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "chHsbrX8dkdQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampling_inference(sentence, encoder, decoder, decoderType=\"LSTM\", embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # input_tensor = torch.Tensor(preprocess_one_hot(sentence))\n",
    "    input_length = len(sentence)\n",
    "\n",
    "    # Initialize encoder & decoder \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_cell = encoder.init_cell()\n",
    "    decoder_hidden = decoder.init_hidden()\n",
    "    decoder_cell = decoder.init_hidden()\n",
    "    encoder_hidden_states = torch.zeros(max_length, encoder.hidden_dim)\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    for ei in range(1,input_length):\n",
    "        # Iteratively run the encoder \n",
    "        # 1. Get the current word index\n",
    "        # 2. Convert to a 1-hot encoding\n",
    "        encoder_input = torch.Tensor(embeddings[word2index[sentence[ei]]]).view(1,1,-1)\n",
    "        # 3. Run one step of the encoder\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "        # 4. Save the encoder hidden states for future processing\n",
    "        encoder_hidden_states[ei,:] = encoder_hidden[0]\n",
    "\n",
    "    # Set the initial hidden and cell state of the RNN decoder to the last \n",
    "    # hidden and cell state of the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "\n",
    "    # Start the decoding with <SOS> token\n",
    "    decoder_input = torch.tensor([word2index.get(\"<SOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "    decoder_input = embeddings[decoder_input]\n",
    "    decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "    EOS = torch.tensor([word2index.get(\"<EOS>\", 0)],dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    word_list = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # Iterate up to the max_length of output\n",
    "    for i in range(max_length):\n",
    "        if decoderType == \"LSTM\": \n",
    "            # Run the simple decoder \n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS:\n",
    "                word_list.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "\n",
    "        if decoderType == \"AttentionLSTM\":\n",
    "            decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell, encoder_output\n",
    "            )\n",
    "            decoder_attentions[i] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi == EOS:\n",
    "              word_list.append('<EOS>')\n",
    "              break\n",
    "            else:\n",
    "              word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "            \n",
    "\n",
    "        if decoderType == \"Transformer\":\n",
    "            decoder_output, encoder_attention_weights, self_attention_weights = decoder(decoder_input,encoder_output.unsqueeze())\n",
    "            topv, topi = decoder_output[0][-1].data.topk(1)\n",
    "            if topi.item() == EOS:\n",
    "              word_list.append('<EOS>')\n",
    "              break\n",
    "            else:\n",
    "              word_list.append(vocabulary[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            decoder_input = embeddings[decoder_input]\n",
    "            decoder_input = torch.tensor(decoder_input, dtype=torch.float).view(1,1,-1)\n",
    "    \n",
    "    res_list = \"\"\n",
    "    for i in range(len(word_list)):\n",
    "        if word_list[i] == \"<EOS>\" or word_list[i] == \"<SOS>\":\n",
    "            continue\n",
    "        else: res_list = res_list + \" \" + word_list[i]\n",
    "\n",
    "\n",
    "    return res_list, decoder_attentions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "_6jeTIFOyfa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: A very clean and well decorated empty bathroom\n",
      "Pig Latin:  away ueblay andway itewhay usbay isway arkedpay\n",
      "Pig Latin:  away ueblay andway itewhay usbay isway arkedpay\n",
      "Pig Latin:  away ueblay andway itewhay usbay isway arkedpay\n",
      "Pig Latin:  away ueblay andway itewhay usbay isway arkedpay\n",
      "Pig Latin:  away ueblay andway itewhay usbay isway arkedpay\n"
     ]
    }
   ],
   "source": [
    "# Lets test it \n",
    "sentence = \"A very clean and well decorated empty bathroom\" \n",
    "input_sentence = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] \n",
    "\n",
    "print(\"English: \" + sentence)\n",
    "\n",
    "for i in range(5):\n",
    "    output_sentence, _ = sampling_inference(input_sentence, encoder, decoder, decoderType=\"LSTM\")\n",
    "    print(\"Pig Latin: \" + output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe3Aq_-VdkdV"
   },
   "source": [
    "## 1.6. Testing \n",
    "\n",
    "We must now define a method that allows us to do inference using the seq2seq architecture. We then run the 500 validation captions through this method, and ultimately compare the **reference** and **generated** sentences using our **BLEU** similarity score method defined above, to identify the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "rcxSh_RWdkdR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distance between \n",
      "  \"Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\" \n",
      "and\n",
      "  \" iewvay ofway away aintray ationstay ithway away argelay itewhay .\" \n",
      " is: 6.373704167435469e-155\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway unchesbay ofway eengray ananasbay onway ananabay eestray.\" \n",
      "and\n",
      "  \" otway ofway irdsbay inway ethay ofway away .\" \n",
      " is: 1.258141043412406e-231\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Anymay alendarscay andway unchesbay ofway ananasbay anginghay onway away allway.\" \n",
      "and\n",
      "  \" eethray irdsbay areway azinggray inway ethay assgray ofway .\" \n",
      " is: 1.0016022933125248e-231\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Ustersclay ofway ananasbay andway icturespay anginghay onway away allway.\" \n",
      "and\n",
      "  \" eoplepay andway away iraffegay alkingway inway ethay assgray .\" \n",
      " is: 1.2387197655613557e-231\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"otway ogsday atthay ooklay otay ebay ightingfay oneway anotherway\" \n",
      "and\n",
      "  \" otway irdsbay areway ittingsay onway ethay assgray .\" \n",
      " is: 9.55980462367717e-232\n",
      "\n",
      "\n",
      "Average BLEU score : 0.022197\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, input_sentence in enumerate(val_sentences):\n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(input_sentence.lower()) + [\"<EOS>\"] \n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "    \n",
    "    output_sentence, _ = inference(input_good, encoder, decoder, decoderType=\"LSTM\")\n",
    "    target_sentence = piglatin_val_sentences[idx]\n",
    "    score = compute_bleu(target_sentence, output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 5 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLEU score : %f\" % (final_score)) \n",
    "\n",
    "\n",
    "# EXPECTED < Average BLUE score (ArgMAX inference): 0.464803 > \n",
    "# EXPECTED < Average BLUE score (sampling inference): 0.477803 > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tL3LKIZ7dkdQ"
   },
   "source": [
    "## 1.7. Experiment with Teacher Forcing\n",
    "\n",
    "Redo steps 1.3 and 1.6 with teacher_forcing_ratio = 0.9 and 0.8. Comment on the results, speed of convergence and the quality of results. Note that in most real scenarious the teacher forcing is actually annealed; starting with teacher forcing = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "DX-D_PI7dkdV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sentence Loss (epoch 0) : 3.804468\n",
      "Single sentence Loss (epoch 0) : 2.774093\n",
      "Single sentence Loss (epoch 0) : 3.157748\n",
      "Single sentence Loss (epoch 0) : 3.333501\n",
      "Single sentence Loss (epoch 0) : 3.362679\n",
      "Single sentence Loss (epoch 0) : 2.586376\n",
      "Single sentence Loss (epoch 0) : 4.486127\n",
      "Single sentence Loss (epoch 0) : 4.485689\n",
      "Single sentence Loss (epoch 0) : 4.302087\n",
      "Single sentence Loss (epoch 0) : 2.670315\n",
      "Loss (epoch 0) : 3.476956\n",
      "Single sentence Loss (epoch 1) : 3.332968\n",
      "Single sentence Loss (epoch 1) : 1.918672\n",
      "Single sentence Loss (epoch 1) : 2.864294\n",
      "Single sentence Loss (epoch 1) : 3.111577\n",
      "Single sentence Loss (epoch 1) : 2.549426\n",
      "Single sentence Loss (epoch 1) : 2.194239\n",
      "Single sentence Loss (epoch 1) : 3.804628\n",
      "Single sentence Loss (epoch 1) : 4.061269\n",
      "Single sentence Loss (epoch 1) : 3.836316\n",
      "Single sentence Loss (epoch 1) : 2.363047\n",
      "Loss (epoch 1) : 2.989311\n",
      "Single sentence Loss (epoch 2) : 3.064690\n",
      "Single sentence Loss (epoch 2) : 1.553361\n",
      "Single sentence Loss (epoch 2) : 2.728071\n",
      "Single sentence Loss (epoch 2) : 2.739921\n",
      "Single sentence Loss (epoch 2) : 2.255412\n",
      "Single sentence Loss (epoch 2) : 1.924330\n",
      "Single sentence Loss (epoch 2) : 3.480507\n",
      "Single sentence Loss (epoch 2) : 3.562977\n",
      "Single sentence Loss (epoch 2) : 3.203631\n",
      "Single sentence Loss (epoch 2) : 2.105298\n",
      "Loss (epoch 2) : 2.703573\n",
      "Single sentence Loss (epoch 3) : 2.953112\n",
      "Single sentence Loss (epoch 3) : 1.297188\n",
      "Single sentence Loss (epoch 3) : 2.386259\n",
      "Single sentence Loss (epoch 3) : 2.317445\n",
      "Single sentence Loss (epoch 3) : 1.925809\n",
      "Single sentence Loss (epoch 3) : 1.501790\n",
      "Single sentence Loss (epoch 3) : 3.283804\n",
      "Single sentence Loss (epoch 3) : 2.961376\n",
      "Single sentence Loss (epoch 3) : 3.052883\n",
      "Single sentence Loss (epoch 3) : 1.697651\n",
      "Loss (epoch 3) : 2.413084\n",
      "Single sentence Loss (epoch 4) : 2.528649\n",
      "Single sentence Loss (epoch 4) : 1.055597\n",
      "Single sentence Loss (epoch 4) : 2.149077\n",
      "Single sentence Loss (epoch 4) : 1.969827\n",
      "Single sentence Loss (epoch 4) : 1.589119\n",
      "Single sentence Loss (epoch 4) : 1.341693\n",
      "Single sentence Loss (epoch 4) : 3.129212\n",
      "Single sentence Loss (epoch 4) : 2.669072\n",
      "Single sentence Loss (epoch 4) : 2.839957\n",
      "Single sentence Loss (epoch 4) : 1.448525\n",
      "Loss (epoch 4) : 2.118524\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "encoder = EncoderLSTM()\n",
    "decoder = DecoderLSTM()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for idx, input_sentence in enumerate(filtered_sentences):\n",
    "        output_sentence = piglatin_filtered_sentences[idx]\n",
    "        loss = train(input_sentence, output_sentence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 0.9, decoderType=\"LSTM\") \n",
    "        count = count + 1\n",
    "        if count % 500 == 0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distance between \n",
      "  \"Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\" \n",
      "and\n",
      "  \" ofway away aintray oinggay oughthray ethay ofway away eetray .\" \n",
      " is: 8.612150057732663e-155\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway unchesbay ofway eengray ananasbay onway ananabay eestray.\" \n",
      "and\n",
      "  \" otway iraffesgay alkingway inway ethay assgray ieldfay .\" \n",
      " is: 1.1368587676511996e-231\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Anymay alendarscay andway unchesbay ofway ananasbay anginghay onway away allway.\" \n",
      "and\n",
      "  \" eoplepay andway away iraffegay andingstay inway ethay assgray .\" \n",
      " is: 1.1084551487393798e-231\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Ustersclay ofway ananasbay andway icturespay anginghay onway away allway.\" \n",
      "and\n",
      "  \" eoplepay onway away enchbay inway ethay iddlemay ofway away .\" \n",
      " is: 6.8489908526642754e-155\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"otway ogsday atthay ooklay otay ebay ightingfay oneway anotherway\" \n",
      "and\n",
      "  \" otway irdsbay areway onway away itycay eetstray .\" \n",
      " is: 9.55980462367717e-232\n",
      "\n",
      "\n",
      "Average BLEU score : 0.017250\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, input_sentence in enumerate(val_sentences):\n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(input_sentence.lower()) + [\"<EOS>\"]\n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "    \n",
    "    output_sentence, _ = inference(input_good, encoder, decoder, decoderType=\"LSTM\")\n",
    "    target_sentence = piglatin_val_sentences[idx]\n",
    "    score = compute_bleu(target_sentence, output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 5 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLEU score : %f\" % (final_score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3wDN2Z1dkdX"
   },
   "source": [
    "## 1.8. Encoding as Generic Feature Representation\n",
    "\n",
    "We now use the final hidden state of our encoder, to identify the nearest neighbor amongst the training sentences for each sentence in our validation data.\n",
    "\n",
    "It would be effective to first define a method that would generate all of the hidden states and store these hidden states **on the CPU**, and then loop over the generated hidden states to identify/output the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "IxFmDA_YdkdY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import device_encoding\n",
    "\n",
    "def final_encoder_hidden(sentence, embeddings=one_hot_embeddings):\n",
    "    # Your code goes here\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_cell = encoder.init_cell()\n",
    "    numerized = preprocess_numberize(sentence)\n",
    "    sentence_one_hot_embeddings = embeddings[numerized]\n",
    "    input_length = sentence_one_hot_embeddings.shape[0]\n",
    "    for ei in range(input_length):\n",
    "        encoder_input = torch.tensor(sentence_one_hot_embeddings[ei,:], dtype=torch.float).view(1,1,-1)\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input , encoder_hidden, encoder_cell)\n",
    "        \n",
    "    return encoder_output.view(-1)\n",
    "# Now run all training data and validation data to store hidden states\n",
    "# Your code goes here\n",
    "val_contexts = []\n",
    "train_contexts = []\n",
    "for sentence in val_sentences:\n",
    "    val_contexts.append(final_encoder_hidden(sentence).cpu().data.numpy())\n",
    "np.save(open('validation_vectors', 'wb+'), val_contexts)\n",
    "    \n",
    "for sentence in train_sentences:\n",
    "    train_contexts.append(final_encoder_hidden(sentence).cpu().data.numpy())\n",
    "np.save(open('training_vectors', 'wb+'), train_contexts)\n",
    "\n",
    "val_contexts = np.load('validation_vectors')\n",
    "train_contexts = np.load('training_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "RpZtVzHpdkdY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(500,)\n",
      "Set of bananas hanging off of a banana tree.\n",
      "Collection of flower pots of various sizes, filled with various flowers.\n",
      "Two bunches of green bananas on banana trees.\n",
      "Two sliced of toasted angel food cake sitting on a white plate.\n",
      "Many calendars and bunches of bananas hanging on a wall.\n",
      "Some animals and a bicycle that are by a house.\n",
      "Clusters of bananas and pictures hanging on a wall.\n",
      "landscape of buildings and a statue in a city\n",
      "two dogs that look to be fighting one another\n",
      "Two discarded toilets face each other on the side of the road\n",
      "Two dogs fighting with one on his back on the ground\n",
      "Two children poised with a fire hydrant on the sidewalk.\n",
      "Bunches of green bananas hanging down from trees.\n",
      "landscape of buildings and a statue in a city\n",
      "Two dogs have a playful fight with one another.\n",
      "Two settler ladies working in a modern day kitchen.\n",
      "Banana trees with large  hanging bunches of bananas.\n",
      "Stoplight with red light on top of monitor\n",
      "There are some green bananas hanging in bunches\n",
      "there are many people boarding a large plane\n"
     ]
    }
   ],
   "source": [
    "# Now get nearest neighbors and print\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distances = euclidean_distances(val_contexts, train_contexts)\n",
    "min_distances = np.argmin(distances, axis=1)\n",
    "print(min_distances.shape)\n",
    "print(min_distances.shape)\n",
    "i = 0\n",
    "for index in min_distances[:10]:\n",
    "    print(val_sentences[i])\n",
    "    i = i+1\n",
    "    print(train_sentences[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFpfb1oi0efe"
   },
   "source": [
    "# Part 2: Attention LSTM Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx2bELJh03hq"
   },
   "source": [
    "## 2.1. Implementing Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "f2adPSOL09Hj"
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_network = nn.Sequential(\n",
    "                                    nn.Linear(hidden_size*2, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, 1)\n",
    "                                 )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the additive attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x 1 x seq_len)\n",
    "\n",
    "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # Your code goes here\n",
    "        # ------------\n",
    "        batch_size = queries.shape[0]\n",
    "        expanded_queries = queries.unsqueeze(1).expand_as(keys)\n",
    "        concat_inputs = torch.cat((expanded_queries, keys), 2)\n",
    "        unnormalized_attention = self.attention_network(concat_inputs) \n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), values.unsqueeze(0))\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOPm_dvX1TrZ"
   },
   "source": [
    "## 2.2. Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "SY7vHNT_1oug"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        self.dropout_p = 0.1\n",
    "        self.linear_input = nn.Linear(wordEncodingSize, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.hidden_dim*2, self.hidden_dim)\n",
    "        self.attention = AdditiveAttention(hidden_size=self.hidden_dim)\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.cell = self.init_cell()\n",
    "\n",
    "    def init_hidden(self):    \n",
    "        return torch.randn(1,1, self.hidden_dim)\n",
    "\n",
    "    def init_cell(self):\n",
    "        return torch.randn(1,1, self.hidden_dim)\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell, encoder_hidden_states):\n",
    "        # ------------\n",
    "        # Your code goes here\n",
    "        # ------------\n",
    "        embed = self.dropout(self.linear_input(input_sentence.view(1,-1)))\n",
    "        context, attention_weights = self.attention(embed, encoder_hidden_states.unsqueeze(0), encoder_hidden_states) \n",
    "        embed_and_context = torch.cat([embed.unsqueeze(0),context], 2) \n",
    "        output, (hidden, cell) = self.lstm(embed_and_context, (hidden, cell))\n",
    "        output = self.linear(output)\n",
    "\n",
    "        return output, hidden , cell, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db7mTRflmQQu"
   },
   "source": [
    "## 2.3. Training Attention Decoder\n",
    "\n",
    "Note that you will need to modify the train() procedure for Part 1 to handles the AttentionLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "EEYM3U0VmdHp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training end to end network ......\n",
      "Single sentence Loss (epoch 0) : 4.010912\n",
      "Single sentence Loss (epoch 0) : 2.348166\n",
      "Single sentence Loss (epoch 0) : 3.094016\n",
      "Single sentence Loss (epoch 0) : 2.861800\n",
      "Single sentence Loss (epoch 0) : 2.664646\n",
      "Single sentence Loss (epoch 0) : 2.606734\n",
      "Single sentence Loss (epoch 0) : 4.368237\n",
      "Single sentence Loss (epoch 0) : 4.211634\n",
      "Single sentence Loss (epoch 0) : 4.111131\n",
      "Single sentence Loss (epoch 0) : 2.682067\n",
      "Loss (epoch 0) : 3.173156\n",
      "Single sentence Loss (epoch 1) : 3.265067\n",
      "Single sentence Loss (epoch 1) : 1.577080\n",
      "Single sentence Loss (epoch 1) : 2.777534\n",
      "Single sentence Loss (epoch 1) : 2.641054\n",
      "Single sentence Loss (epoch 1) : 2.312160\n",
      "Single sentence Loss (epoch 1) : 2.086554\n",
      "Single sentence Loss (epoch 1) : 3.881687\n",
      "Single sentence Loss (epoch 1) : 3.465160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [129], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(filtered_sentences):\n\u001b[1;32m     14\u001b[0m     target_variable \u001b[38;5;241m=\u001b[39m piglatin_filtered_sentences[\u001b[38;5;28mid\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_variable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoderType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAttentionLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     count \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m count\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m500\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn [118], line 137\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_sentence, output_sentence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio, decoderType, embeddings)\u001b[0m\n\u001b[1;32m    134\u001b[0m     loss\u001b[38;5;241m=\u001b[39mcriterion(pred[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], output_tensor[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Your code goes here\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    139\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/pixplot/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pixplot/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = AttentionDecoder()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end to end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, sentence in enumerate(filtered_sentences):\n",
    "        target_variable = piglatin_filtered_sentences[id]\n",
    "        loss = train(sentence, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"AttentionLSTM\")\n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFTuFon2m7f9"
   },
   "source": [
    "## 2.4. Testing Attention Decoder\n",
    "Note that you will need to modify the inference() procedure for Part 1 to handle Attention LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "yx3flyVdnGCg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuanchen/miniforge3/envs/pixplot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/xuanchen/miniforge3/envs/pixplot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/xuanchen/miniforge3/envs/pixplot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distance between \n",
      "  \"Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\" \n",
      "and\n",
      "  \" erethay areway away usbay inway away assygray areaway .\" \n",
      " is: 1.1193096620723278e-231\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway unchesbay ofway eengray ananasbay onway ananabay eestray.\" \n",
      "and\n",
      "  \" erethay areway away affictray ightlay inway ethay ackgroundbay\" \n",
      " is: 0\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Anymay alendarscay andway unchesbay ofway ananasbay anginghay onway away allway.\" \n",
      "and\n",
      "  \" erethay areway away usbay inway away ieldfay .\" \n",
      " is: 8.853864984883467e-232\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Ustersclay ofway ananasbay andway icturespay anginghay onway away allway.\" \n",
      "and\n",
      "  \" erethay isway away usbay inway away ieldfay .\" \n",
      " is: 1.0032743411283238e-231\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"otway ogsday atthay ooklay otay ebay ightingfay oneway anotherway\" \n",
      "and\n",
      "  \" erethay areway away usbay inway ethay ackgroundbay .\" \n",
      " is: 0\n",
      "\n",
      "\n",
      "Average BLEU score : 0.001363\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, input_sentence in enumerate(val_sentences):\n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(input_sentence.lower()) + [\"<EOS>\"] \n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "    \n",
    "    output_sentence, _ = inference(input_good, encoder, decoder, decoderType=\"AttentionLSTM\")\n",
    "    target_sentence = piglatin_val_sentences[idx]\n",
    "    score = compute_bleu(target_sentence, output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 5 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLEU score : %f\" % (final_score))\n",
    "\n",
    "# EXPECTED < Average BLUE score (ArgMAX inference): 0.739589 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvKl0zhhngVg"
   },
   "source": [
    "## 2.5. Visualize Attention for Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "tsAfscbVnhF6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distnace between \n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "and\n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "is: 1.0\n",
      "\n",
      "\n",
      "BLEU score distnace between \n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "and\n",
      "  \"A few people sit on a dim transportation system. \" \n",
      "is: 0.1933853138176172\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +['<EOS>'])\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "score1 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[0])\n",
    "score2 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[5])\n",
    "\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[0] + '\" \\nis: ' + str(score1) +'\\n\\n')\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[5] + '\" \\nis: ' + str(score2) +'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XABkHOBJns76"
   },
   "source": [
    "# Part 3: Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFb5tCzOpDmB"
   },
   "source": [
    "## 3.1 Implement Scaled Dot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDgSOIvdpCuR"
   },
   "outputs": [],
   "source": [
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # Your code goes here\n",
    "        # ------------\n",
    "        batch_size = queries.size(0)\n",
    "        if queries.dim() != 3:\n",
    "            queries = torch.unsqueeze(queries, dim = 1)\n",
    "        q = self.Q(queries)\n",
    "        k = self.K(keys)\n",
    "        k = torch.transpose(k, 1, 2)\n",
    "        v = self.V(values)\n",
    "        unnormalized_attention = torch.bmm(q,k) * self.scaling_factor\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights, v)\n",
    "\n",
    "        return context, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4gpSwDopbHZ"
   },
   "source": [
    "## 3.2. Implement Causal Scaled Dot Attention\n",
    "\n",
    "The implementation should be nearly identical to the one above, but with mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "ZFnGpHklpbpQ"
   },
   "outputs": [],
   "source": [
    "class CausalScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.neg_inf = torch.tensor(-1e7)\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        NOTES:\n",
    "            batch_size = 1\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "                In training k = maxSequenceLength or length of the GT ourput sequence\n",
    "                In testing k = length of currently decoded sub-sequence\n",
    "            keys: The decoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The decoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # Your code goes here\n",
    "        # ------------\n",
    "        batch_size = queries.size(0)\n",
    "\n",
    "        k_value = queries.size(1)\n",
    "        seq_len = keys.size(1)\n",
    "\n",
    "        if queries.dim() != 3:\n",
    "            queries = torch.unsqueeze(queries, dim = 1)\n",
    "        q = self.Q(queries)\n",
    "        k = self.K(keys)\n",
    "        k = torch.transpose(k, 1, 2)\n",
    "        v = self.V(values)\n",
    "        unnormalized_attention = torch.bmm(q,k) * self.scaling_factor\n",
    "        mask = torch.tril(torch.ones(batch_size, k_value, seq_len))\n",
    "        unnormalized_attention[mask == 0] = self.neg_inf\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights, v)\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsLrprNLqQ5x"
   },
   "source": [
    "## 3.3. Implement Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "FjiwfUHXqRXI"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        self.dropout_p = 0.1\n",
    "        self.num_layers = 3\n",
    "        self.linear_input = nn.Linear(wordEncodingSize, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
    "                                    hidden_size=self.hidden_dim, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=self.hidden_dim, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                                    nn.ReLU(),\n",
    "                                 ) for i in range(self.num_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell, annotations):\n",
    "        embed = self.dropout(self.linear_input(input_sentence)).unsqueeze(0)\n",
    "        \n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        batch_size, seq_len, hidden_size = contexts.size()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
    "            residual_contexts = contexts + new_contexts\n",
    "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts,annotations.unsqueeze(0), annotations.unsqueeze(0))\n",
    "            residual_contexts = residual_contexts + new_contexts\n",
    "            new_contexts = self.attention_mlps[i](residual_contexts.view(-1, self.hidden_dim)).view(batch_size, seq_len, self.hidden_dim)\n",
    "            contexts = residual_contexts + new_contexts\n",
    "            \n",
    "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "            self_attention_weights_list.append(self_attention_weights)            \n",
    "        \n",
    "        output = self.linear(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "        \n",
    "        return output, encoder_attention_weights, self_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENolyKTWrAWq"
   },
   "source": [
    "## 3.4. Training Transformer Decoder\n",
    "\n",
    "Note that you will need to modify the train() procedure for Part 1 to handle the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "1iuUnzkZrA44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training end to end network ......\n",
      "Single sentence Loss (epoch 0) : 0.419194\n",
      "Single sentence Loss (epoch 0) : 0.163597\n",
      "Single sentence Loss (epoch 0) : 0.220472\n",
      "Single sentence Loss (epoch 0) : 0.178288\n",
      "Single sentence Loss (epoch 0) : 0.196344\n",
      "Single sentence Loss (epoch 0) : 0.147067\n",
      "Single sentence Loss (epoch 0) : 0.257912\n",
      "Single sentence Loss (epoch 0) : 0.342656\n",
      "Single sentence Loss (epoch 0) : 0.196589\n",
      "Single sentence Loss (epoch 0) : 0.057299\n",
      "Loss (epoch 0) : 0.199598\n",
      "Single sentence Loss (epoch 1) : 0.167105\n",
      "Single sentence Loss (epoch 1) : 0.004150\n",
      "Single sentence Loss (epoch 1) : 0.036919\n",
      "Single sentence Loss (epoch 1) : 0.044097\n",
      "Single sentence Loss (epoch 1) : 0.000772\n",
      "Single sentence Loss (epoch 1) : 0.047090\n",
      "Single sentence Loss (epoch 1) : 0.071687\n",
      "Single sentence Loss (epoch 1) : 0.078573\n",
      "Single sentence Loss (epoch 1) : 0.013199\n",
      "Single sentence Loss (epoch 1) : 0.040677\n",
      "Loss (epoch 1) : 0.047187\n",
      "Single sentence Loss (epoch 2) : 0.071531\n",
      "Single sentence Loss (epoch 2) : 0.000074\n",
      "Single sentence Loss (epoch 2) : 0.014427\n",
      "Single sentence Loss (epoch 2) : 0.022081\n",
      "Single sentence Loss (epoch 2) : 0.000894\n",
      "Single sentence Loss (epoch 2) : 0.018679\n",
      "Single sentence Loss (epoch 2) : 0.030093\n",
      "Single sentence Loss (epoch 2) : 0.012909\n",
      "Single sentence Loss (epoch 2) : 0.000710\n",
      "Single sentence Loss (epoch 2) : 0.000249\n",
      "Loss (epoch 2) : 0.016217\n",
      "Single sentence Loss (epoch 3) : 0.037876\n",
      "Single sentence Loss (epoch 3) : 0.001509\n",
      "Single sentence Loss (epoch 3) : 0.000480\n",
      "Single sentence Loss (epoch 3) : 0.000931\n",
      "Single sentence Loss (epoch 3) : 0.000102\n",
      "Single sentence Loss (epoch 3) : 0.000151\n",
      "Single sentence Loss (epoch 3) : 0.033923\n",
      "Single sentence Loss (epoch 3) : 0.007406\n",
      "Single sentence Loss (epoch 3) : 0.000011\n",
      "Single sentence Loss (epoch 3) : 0.000110\n",
      "Loss (epoch 3) : 0.010085\n",
      "Single sentence Loss (epoch 4) : 0.002338\n",
      "Single sentence Loss (epoch 4) : 0.000014\n",
      "Single sentence Loss (epoch 4) : 0.061223\n",
      "Single sentence Loss (epoch 4) : 0.000070\n",
      "Single sentence Loss (epoch 4) : 0.010933\n",
      "Single sentence Loss (epoch 4) : 0.000036\n",
      "Single sentence Loss (epoch 4) : 0.000147\n",
      "Single sentence Loss (epoch 4) : 0.000086\n",
      "Single sentence Loss (epoch 4) : 0.000009\n",
      "Single sentence Loss (epoch 4) : 0.000573\n",
      "Loss (epoch 4) : 0.004499\n"
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = TransformerDecoder()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end to end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, sentence in enumerate(filtered_sentences):\n",
    "        target_variable = piglatin_filtered_sentences[id]\n",
    "        loss = train(sentence, target_variable, encoder, decoder,encoder_optimizer,decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"Transformer\")\n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOF6M3tWrbOS"
   },
   "source": [
    "## 3.5. Testing Transformer Decoder\n",
    "Note that you will need to modify the inference() procedure for Part 1 to handle Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "g2BlDGfcroOA"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [168], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m input_tok \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<SOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m word_tokenize(input_sentence\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<EOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[1;32m      7\u001b[0m input_good \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m input_tok \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word2index]\n\u001b[0;32m----> 9\u001b[0m output_sentence, _ \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_good\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoderType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m target_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<SOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39moutput_sentence\n\u001b[1;32m     11\u001b[0m score \u001b[38;5;241m=\u001b[39m compute_bleu(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<SOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtarget_sentence[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<SOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39moutput_sentence)\n",
      "Cell \u001b[0;32mIn [167], line 73\u001b[0m, in \u001b[0;36minference\u001b[0;34m(sentence, encoder, decoder, decoderType, embeddings, max_length)\u001b[0m\n\u001b[1;32m     70\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(decoder_input, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoderType \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m     decoder_output, encoder_decoder_attn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_cell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     decoder_attentions \u001b[38;5;241m=\u001b[39m encoder_decoder_attn\n\u001b[1;32m     75\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m decoder_output[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:,:]\n",
      "File \u001b[0;32m~/miniforge3/envs/pixplot/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [148], line 31\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, input_sentence, hidden, cell, annotations)\u001b[0m\n\u001b[1;32m     29\u001b[0m self_attention_weights_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m contexts \u001b[38;5;241m=\u001b[39m embed\n\u001b[0;32m---> 31\u001b[0m batch_size, seq_len, hidden_size \u001b[38;5;241m=\u001b[39m contexts\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m     34\u001b[0m     new_contexts, self_attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attentions[i](contexts, contexts, contexts)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, input_sentence in enumerate(val_sentences): \n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(input_sentence.lower()) + [\"<EOS>\"] \n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "    \n",
    "    output_sentence, _ = inference(input_good, encoder, decoder, decoderType=\"Transformer\")\n",
    "    target_sentence = \"<SOS>\"+output_sentence\n",
    "    score = compute_bleu(\"<SOS>\"+target_sentence[0], \"<SOS>\"+output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 10 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLEU score : %f\" % (final_score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCrOGW2NrzPG"
   },
   "source": [
    "## 3.6 Visualizing Attention for Transformer Decoder\n",
    "\n",
    "Note that since we have multiple attention layers, there will be one attention to be visualized per layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distnace between \n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "and\n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "is: 1.0\n",
      "\n",
      "\n",
      "BLEU score distnace between \n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "and\n",
      "  \"A few people sit on a dim transportation system. \" \n",
      "is: 0.1933853138176172\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +['<EOS>'])\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "score1 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[0])\n",
    "score2 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[5])\n",
    "\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[0] + '\" \\nis: ' + str(score1) +'\\n\\n')\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[5] + '\" \\nis: ' + str(score2) +'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jng5Ksadkdp"
   },
   "source": [
    "# 4. Effectiveness of word2vec\n",
    "\n",
    "As an option, you may repeat one of the models above by modifying the code to use word2vec embedding for the input English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RWVVvgyudkdq",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.12 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
