{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnzrOsd0bZHS"
   },
   "source": [
    "# CPSC532S Assignment 3:  RNNs for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t_cnRUY7dkdI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oSmx8s7dkdK"
   },
   "source": [
    "# Data Acquisition\n",
    "\n",
    "\n",
    "The goal of this assignment is to translate English to Pig Latin. For this assignment, you must download the data and extract it into `data/`. The dataset contains four files, each containing a single caption on each line. There are two files for training (English vs Pig Latin) and two files for validation. We should have 20,000 sentences (one sentence per image in Assignment 2) in the training captions and 500 sentences in the validation captions (five sentences per image in Assignment 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS3DYtiqdkdL",
    "outputId": "2ba0eebc-7234-481c-e0e2-8931bf0ff8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "500\n",
      "20000\n",
      "500\n",
      "A very clean and well decorated empty bathroom\n",
      "Away eryvay eanclay andway ellway ecoratedday emptyway athroombay\n",
      "Set of bananas hanging off of a banana tree.\n",
      "Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Load the data into memory.\n",
    "mscoco_train = json.load(open(\"./content/train_captions.json\"))\n",
    "mscoco_val  = json.load(open('./content/val_captions.json'))\n",
    "\n",
    "mscoco_piglatin_train = json.load(open('./content/piglatin_train_captions.json'))\n",
    "mscoco_piglatin_val  = json.load(open('./content/piglatin_val_captions.json'))\n",
    "\n",
    "train_sentences = [entry['caption'] for entry in mscoco_train['annotations']]\n",
    "val_sentences = [entry['caption'] for entry in mscoco_val['annotations']]\n",
    "\n",
    "piglatin_train_sentences = [entry['caption'] for entry in mscoco_piglatin_train['annotations']]\n",
    "piglatin_val_sentences = [entry['caption'] for entry in mscoco_piglatin_val['annotations']]\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(piglatin_train_sentences))\n",
    "print(len(piglatin_val_sentences))\n",
    "print(train_sentences[0])\n",
    "print(piglatin_train_sentences[0])\n",
    "print(val_sentences[0])\n",
    "print(piglatin_val_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyGm8VmCdkdM"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "The code provided below creates word embeddings for you to use. After creating the vocabulary, we construct both one-hot embeddings and word2vec embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYWipM4tdkdN",
    "outputId": "eb4c7879-7d76-4d1c-a032-61b9008568bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xuanchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = train_sentences\n",
    "piglatin_sentences = piglatin_train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "piglatin_sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in piglatin_sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 2000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "piglatin_word_counts = Counter([word for sentence in piglatin_sentences for word in sentence])\n",
    "word_counts = word_counts + piglatin_word_counts\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "\n",
    "# Build the one hot embeddings\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "piglatin_filtered_sentences = [[word for word in sentence if word in word2index] for sentence in piglatin_sentences]\n",
    "all_filtered_sentences = filtered_sentences + piglatin_filtered_sentences\n",
    "w2v = Word2Vec(all_filtered_sentences, min_count=0, vector_size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.vectors))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "piglatin_maxSequenceLength = max([len(sentence) for sentence in piglatin_sentences])\n",
    "\n",
    "if piglatin_maxSequenceLength > maxSequenceLength:\n",
    "    maxSequenceLength = piglatin_maxSequenceLength\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zYnaPC0i0A2"
   },
   "source": [
    "# Utilities functions\n",
    "\n",
    "\n",
    "Please look through the functions provided below carefully, as you will need to use all of them at some point in your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PFOm2o3jINX",
    "outputId": "24652362-a696-49a8-e2de-f933f0ae64a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distnace between \n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "and\n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "is: 1.0\n",
      "\n",
      "\n",
      "BLEU score distnace between \n",
      "  \"A very clean and well decorated empty bathroom\" \n",
      "and\n",
      "  \"A few people sit on a dim transportation system. \" \n",
      "is: 0.1933853138176172\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentence, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a reference sentence, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = word_tokenize(reference_sentence.lower())\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu([reference_tokenized], predicted_tokenized)\n",
    "\n",
    "%matplotlib inline\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +['<EOS>'])\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "score1 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[0])\n",
    "score2 = compute_bleu(\"<SOS>\" + train_sentences[0], \"<SOS>\" + train_sentences[5])\n",
    "\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[0] + '\" \\nis: ' + str(score1) +'\\n\\n')\n",
    "print('BLEU score distnace between \\n  \"' + train_sentences[0] + '\" \\nand\\n  \"'+ train_sentences[5] + '\" \\nis: ' + str(score2) +'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZD-r_MckSLA"
   },
   "source": [
    "#Part 1: Encoder-Decoder Language Translation with Teacher-Forcing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSOKOJModkdN"
   },
   "source": [
    "## 1.1 Building a Language Decoder\n",
    "\n",
    "We now implement a language decoder. For now, we will have the decoder take a single training sample at a time (as opposed to batching). For our purposes, we will also avoid defining the embeddings as part of the model and instead pass in embedded inputs. While this is sometimes useful, as it learns/tunes the embeddings, we avoid doing it for the sake of simplicity and speed.\n",
    "\n",
    "Remember to use LSTM hidden units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "w7g1DsQcdkdO",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        \n",
    "        self.lstm = nn.LSTM(wordEncodingSize, self.hidden_dim)\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.cell = self.init_cell()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.randn(1,1, self.hidden_dim).cuda()\n",
    "\n",
    "\n",
    "    def init_cell(self):\n",
    "        return torch.randn(1,1, self.hidden_dim).cuda()\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell):\n",
    "        lstm_in = input_sentence.view(1,1,-1).cuda()\n",
    "        output, (hidden, cell) = self.lstm(lstm_in, (hidden, cell))\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJVTRdUkdkdR"
   },
   "source": [
    "## 1.2.  Building Language Encoder\n",
    "\n",
    "We now build a language encoder, which will encode an input word by word, and ultimately output a hidden state that we can then be used by our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "kccifunUdkdR",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        \n",
    "        self.lstm = nn.LSTM(wordEncodingSize, self.hidden_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.cell = self.init_cell()\n",
    "       \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim).cuda()\n",
    "\n",
    "    def init_cell(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim).cuda()\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell):\n",
    "        lstm_in = input_sentence.view(1,1,-1).cuda()\n",
    "        output, (hidden, cell) = self.lstm(lstm_in, (hidden, cell))\n",
    "        \n",
    "        return output, hidden, cell\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljBX3m0tdkdT"
   },
   "source": [
    "## 1.3. Connecting Encoder to Decoder and Train End-to-End and Train with Teacher Forcing\n",
    "\n",
    "We now connect our newly created encoder with our decoder, to train an end-to-end seq2seq architecture. \n",
    "\n",
    "For the purposes of Part 1, the only interaction between the encoder and the decoder is that the *last hidden state of the encoder is used as the initial hidden state of the decoder*. This will be different for Part 2 and 3 where we will extend this punction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "kN5nFk_ndkdU",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(input_sentence, output_sentence, encoder,\n",
    "          decoder, encoder_optimizer,\n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          teacher_forcing_ratio = 1,\n",
    "          decoderType = \"LSTM\",\n",
    "          embeddings = one_hot_embeddings): \n",
    "    \"\"\"\n",
    "    Given a single training sample, go through a single step of training.\n",
    "    \"\"\"\n",
    "    use_teacher_forcing = True if np.random.rand() < teacher_forcing_ratio else False\n",
    "    \n",
    "    # Lengths of input / output\n",
    "    input_len  = len(input_sentence)\n",
    "    output_len = len(output_sentence)\n",
    "\n",
    "    # Preliminaries of encoder / decoder optimization\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Initialize output prediction layer\n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Encoder hidden / cell states \n",
    "    encoder_hidden = encoder.hidden\n",
    "    encoder_cell   = encoder.cell \n",
    "    \n",
    "    # Initialize encoder hidden states (for Attention LSTM decoder and/or Transforemer decoder)\n",
    "    encoder_hidden_states = torch.zeros(maxSequenceLength, encoder.hidden_dim)\n",
    "\n",
    "    # Iterate over the input sequence \n",
    "    for ei in range(1, input_len):\n",
    "        # Get the current word index\n",
    "        word_idx = word2index[input_sentence[ei]]\n",
    "        # Convert to a 1-hot encoding\n",
    "        encoder_input = torch.Tensor(embeddings[word_idx])\n",
    "        # Run one step of the encoder\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "        # Save the encoder hidden states for future processing\n",
    "        encoder_hidden_states[ei,:] = encoder_hidden[0]\n",
    "\n",
    "    # Setup decoder to start from the <SOS> token\n",
    "    word_indx = word2index[\"<SOS>\"]\n",
    "    decoder_input = torch.FloatTensor(embeddings[word_indx]).cuda()\n",
    "    \n",
    "    if decoderType == \"Transformer\":\n",
    "        full_sentence_decoder_input = torch.zeros(output_len, vocabularySize).cuda()\n",
    "        full_sentence_decoder_input[0,:] = decoder_input.unsqueeze(0)\n",
    "        full_sentence_target_output = torch.zeros(output_len, dtype=torch.int64).cuda() \n",
    "\n",
    "    # Initialize decoder hidden/cell state to the last encoder hidden/cell state\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = 0   \n",
    "    \n",
    "    for i in range(1, output_len):\n",
    "        if decoderType == \"LSTM\": \n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "\n",
    "        if decoderType == \"AttentionLSTM\": \n",
    "            decoder_output, decoder_hidden, decoder_cell, attention_weights = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_hidden_states.cuda())\n",
    "\n",
    "        # Designate the next target word index\n",
    "        next_word = output_sentence[i]\n",
    "        next_word_indx = word2index[next_word]\n",
    "        decoder_word_input = embeddings[next_word_indx]\n",
    "        target_output = torch.LongTensor(np.nonzero(decoder_word_input)[0]).cuda()\n",
    "\n",
    "        if decoderType == \"Transformer\":\n",
    "            full_sentence_decoder_input[i,:] = torch.FloatTensor(decoder_word_input).cuda()\n",
    "            full_sentence_target_output[i-1] = torch.LongTensor(np.nonzero(decoder_word_input)[0]) \n",
    "\n",
    "        if use_teacher_forcing: \n",
    "            decoder_input = torch.FloatTensor(decoder_word_input).cuda()\n",
    "        elif not use_teacher_forcing:\n",
    "            topv, topi = log_softmax(decoder_output[0]).topk(1)\n",
    "            decoder_input = torch.FloatTensor(embeddings[topi.squeeze().detach().item()]).cuda() \n",
    "\n",
    "        # Compute the loss if not a Transformer\n",
    "        if decoderType != \"Transformer\":     \n",
    "          loss += criterion(decoder_output[0], target_output)\n",
    "\n",
    "        if not use_teacher_forcing:\n",
    "            if topi.squeeze().item() == word2index[\"<EOS>\"]:\n",
    "                break     \n",
    "\n",
    "    # Compute the loss for a Transformer\n",
    "    if decoderType == \"Transformer\":\n",
    "        full_sentence_decoder_output, self_attn, encoder_decoder_attn = decoder(full_sentence_decoder_input, decoder_hidden, decoder_cell, encoder_hidden_states.cuda())\n",
    "        loss += criterion(full_sentence_decoder_output[0], full_sentence_target_output) \n",
    "\n",
    "    # Backpropagate gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Make optimizer step\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # Average the loss\n",
    "    final_loss = loss.item() / output_len\n",
    "\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaEuQm7KwHBJ",
    "outputId": "727972fb-471d-47c5-bfde-8e179de3fe3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training end-to-end network ......\n",
      "Single sentence Loss (epoch 0) : 3.083224\n",
      "Single sentence Loss (epoch 0) : 2.743134\n",
      "Single sentence Loss (epoch 0) : 3.105756\n",
      "Single sentence Loss (epoch 0) : 3.782783\n",
      "Single sentence Loss (epoch 0) : 3.300224\n",
      "Single sentence Loss (epoch 0) : 1.948817\n",
      "Single sentence Loss (epoch 0) : 3.921243\n",
      "Single sentence Loss (epoch 0) : 4.360262\n",
      "Single sentence Loss (epoch 0) : 4.042925\n",
      "Single sentence Loss (epoch 0) : 2.636833\n",
      "Single sentence Loss (epoch 0) : 2.925396\n",
      "Single sentence Loss (epoch 0) : 3.286050\n",
      "Single sentence Loss (epoch 0) : 4.190079\n",
      "Single sentence Loss (epoch 0) : 1.854980\n",
      "Single sentence Loss (epoch 0) : 4.335175\n",
      "Single sentence Loss (epoch 0) : 2.931729\n",
      "Single sentence Loss (epoch 0) : 3.258193\n",
      "Single sentence Loss (epoch 0) : 3.178576\n",
      "Single sentence Loss (epoch 0) : 2.582512\n",
      "Single sentence Loss (epoch 0) : 4.997442\n",
      "Single sentence Loss (epoch 0) : 2.929589\n",
      "Single sentence Loss (epoch 0) : 2.532119\n",
      "Single sentence Loss (epoch 0) : 2.615602\n",
      "Single sentence Loss (epoch 0) : 4.377149\n",
      "Single sentence Loss (epoch 0) : 3.988008\n",
      "Single sentence Loss (epoch 0) : 1.449586\n",
      "Single sentence Loss (epoch 0) : 3.035845\n",
      "Single sentence Loss (epoch 0) : 2.324415\n",
      "Single sentence Loss (epoch 0) : 1.934094\n",
      "Single sentence Loss (epoch 0) : 2.806140\n",
      "Single sentence Loss (epoch 0) : 1.660239\n",
      "Single sentence Loss (epoch 0) : 2.518527\n",
      "Single sentence Loss (epoch 0) : 2.302077\n",
      "Single sentence Loss (epoch 0) : 1.748448\n",
      "Single sentence Loss (epoch 0) : 2.525727\n",
      "Single sentence Loss (epoch 0) : 2.103398\n",
      "Single sentence Loss (epoch 0) : 1.084725\n",
      "Single sentence Loss (epoch 0) : 2.646084\n",
      "Single sentence Loss (epoch 0) : 2.573986\n",
      "Single sentence Loss (epoch 0) : 4.469753\n",
      "Loss (epoch 0) : 2.719388\n",
      "Single sentence Loss (epoch 1) : 1.914158\n",
      "Single sentence Loss (epoch 1) : 0.887350\n",
      "Single sentence Loss (epoch 1) : 2.234013\n",
      "Single sentence Loss (epoch 1) : 2.687346\n",
      "Single sentence Loss (epoch 1) : 1.856245\n",
      "Single sentence Loss (epoch 1) : 0.894271\n",
      "Single sentence Loss (epoch 1) : 2.926102\n",
      "Single sentence Loss (epoch 1) : 3.448040\n",
      "Single sentence Loss (epoch 1) : 3.021406\n",
      "Single sentence Loss (epoch 1) : 1.780942\n",
      "Single sentence Loss (epoch 1) : 2.384584\n",
      "Single sentence Loss (epoch 1) : 2.627972\n",
      "Single sentence Loss (epoch 1) : 3.432858\n",
      "Single sentence Loss (epoch 1) : 1.278136\n",
      "Single sentence Loss (epoch 1) : 3.126482\n",
      "Single sentence Loss (epoch 1) : 2.402460\n",
      "Single sentence Loss (epoch 1) : 2.366993\n",
      "Single sentence Loss (epoch 1) : 2.830942\n",
      "Single sentence Loss (epoch 1) : 0.934948\n",
      "Single sentence Loss (epoch 1) : 2.634489\n",
      "Single sentence Loss (epoch 1) : 2.108891\n",
      "Single sentence Loss (epoch 1) : 1.793988\n",
      "Single sentence Loss (epoch 1) : 1.556425\n",
      "Single sentence Loss (epoch 1) : 3.022305\n",
      "Single sentence Loss (epoch 1) : 2.901046\n",
      "Single sentence Loss (epoch 1) : 0.741720\n",
      "Single sentence Loss (epoch 1) : 1.866100\n",
      "Single sentence Loss (epoch 1) : 2.117472\n",
      "Single sentence Loss (epoch 1) : 1.135681\n",
      "Single sentence Loss (epoch 1) : 2.572060\n",
      "Single sentence Loss (epoch 1) : 0.976913\n",
      "Single sentence Loss (epoch 1) : 1.161120\n",
      "Single sentence Loss (epoch 1) : 1.120004\n",
      "Single sentence Loss (epoch 1) : 1.311168\n",
      "Single sentence Loss (epoch 1) : 1.470937\n",
      "Single sentence Loss (epoch 1) : 1.333866\n",
      "Single sentence Loss (epoch 1) : 0.532364\n",
      "Single sentence Loss (epoch 1) : 1.978021\n",
      "Single sentence Loss (epoch 1) : 1.576482\n",
      "Single sentence Loss (epoch 1) : 2.785998\n",
      "Loss (epoch 1) : 1.822524\n",
      "Single sentence Loss (epoch 2) : 1.172825\n",
      "Single sentence Loss (epoch 2) : 0.389177\n",
      "Single sentence Loss (epoch 2) : 1.245830\n",
      "Single sentence Loss (epoch 2) : 1.998774\n",
      "Single sentence Loss (epoch 2) : 1.060473\n",
      "Single sentence Loss (epoch 2) : 0.669212\n",
      "Single sentence Loss (epoch 2) : 2.272044\n",
      "Single sentence Loss (epoch 2) : 2.602277\n",
      "Single sentence Loss (epoch 2) : 2.063655\n",
      "Single sentence Loss (epoch 2) : 0.941597\n",
      "Single sentence Loss (epoch 2) : 1.913551\n",
      "Single sentence Loss (epoch 2) : 2.142399\n",
      "Single sentence Loss (epoch 2) : 2.663849\n",
      "Single sentence Loss (epoch 2) : 0.982056\n",
      "Single sentence Loss (epoch 2) : 2.375313\n",
      "Single sentence Loss (epoch 2) : 1.413251\n",
      "Single sentence Loss (epoch 2) : 1.423465\n",
      "Single sentence Loss (epoch 2) : 1.700889\n",
      "Single sentence Loss (epoch 2) : 0.529222\n",
      "Single sentence Loss (epoch 2) : 1.653732\n",
      "Single sentence Loss (epoch 2) : 1.636689\n",
      "Single sentence Loss (epoch 2) : 1.458044\n",
      "Single sentence Loss (epoch 2) : 0.961678\n",
      "Single sentence Loss (epoch 2) : 2.357360\n",
      "Single sentence Loss (epoch 2) : 1.984094\n",
      "Single sentence Loss (epoch 2) : 0.288984\n",
      "Single sentence Loss (epoch 2) : 1.103707\n",
      "Single sentence Loss (epoch 2) : 1.389142\n",
      "Single sentence Loss (epoch 2) : 0.602838\n",
      "Single sentence Loss (epoch 2) : 1.867098\n",
      "Single sentence Loss (epoch 2) : 0.611282\n",
      "Single sentence Loss (epoch 2) : 0.771736\n",
      "Single sentence Loss (epoch 2) : 0.704518\n",
      "Single sentence Loss (epoch 2) : 1.150864\n",
      "Single sentence Loss (epoch 2) : 1.105448\n",
      "Single sentence Loss (epoch 2) : 1.201599\n",
      "Single sentence Loss (epoch 2) : 0.342649\n",
      "Single sentence Loss (epoch 2) : 1.481612\n",
      "Single sentence Loss (epoch 2) : 1.394928\n",
      "Single sentence Loss (epoch 2) : 1.838804\n",
      "Loss (epoch 2) : 1.239626\n",
      "Single sentence Loss (epoch 3) : 1.046173\n",
      "Single sentence Loss (epoch 3) : 0.146875\n",
      "Single sentence Loss (epoch 3) : 1.027500\n",
      "Single sentence Loss (epoch 3) : 1.454960\n",
      "Single sentence Loss (epoch 3) : 0.498383\n",
      "Single sentence Loss (epoch 3) : 0.188646\n",
      "Single sentence Loss (epoch 3) : 1.641419\n",
      "Single sentence Loss (epoch 3) : 2.309307\n",
      "Single sentence Loss (epoch 3) : 1.342128\n",
      "Single sentence Loss (epoch 3) : 0.588446\n",
      "Single sentence Loss (epoch 3) : 1.647629\n",
      "Single sentence Loss (epoch 3) : 1.393603\n",
      "Single sentence Loss (epoch 3) : 1.745711\n",
      "Single sentence Loss (epoch 3) : 0.587772\n",
      "Single sentence Loss (epoch 3) : 1.938325\n",
      "Single sentence Loss (epoch 3) : 0.895132\n",
      "Single sentence Loss (epoch 3) : 1.037594\n",
      "Single sentence Loss (epoch 3) : 0.806116\n",
      "Single sentence Loss (epoch 3) : 0.157060\n",
      "Single sentence Loss (epoch 3) : 1.106355\n",
      "Single sentence Loss (epoch 3) : 1.218840\n",
      "Single sentence Loss (epoch 3) : 0.893003\n",
      "Single sentence Loss (epoch 3) : 0.475169\n",
      "Single sentence Loss (epoch 3) : 1.975774\n",
      "Single sentence Loss (epoch 3) : 1.156859\n",
      "Single sentence Loss (epoch 3) : 0.072806\n",
      "Single sentence Loss (epoch 3) : 0.593560\n",
      "Single sentence Loss (epoch 3) : 0.992384\n",
      "Single sentence Loss (epoch 3) : 0.321621\n",
      "Single sentence Loss (epoch 3) : 1.482996\n",
      "Single sentence Loss (epoch 3) : 0.344344\n",
      "Single sentence Loss (epoch 3) : 0.690715\n",
      "Single sentence Loss (epoch 3) : 0.350583\n",
      "Single sentence Loss (epoch 3) : 0.882858\n",
      "Single sentence Loss (epoch 3) : 0.424304\n",
      "Single sentence Loss (epoch 3) : 0.426907\n",
      "Single sentence Loss (epoch 3) : 0.071680\n",
      "Single sentence Loss (epoch 3) : 1.003663\n",
      "Single sentence Loss (epoch 3) : 1.193500\n",
      "Single sentence Loss (epoch 3) : 1.573775\n",
      "Loss (epoch 3) : 0.835653\n",
      "Single sentence Loss (epoch 4) : 0.478604\n",
      "Single sentence Loss (epoch 4) : 0.109051\n",
      "Single sentence Loss (epoch 4) : 0.629640\n",
      "Single sentence Loss (epoch 4) : 0.962482\n",
      "Single sentence Loss (epoch 4) : 0.322878\n",
      "Single sentence Loss (epoch 4) : 0.063031\n",
      "Single sentence Loss (epoch 4) : 0.937352\n",
      "Single sentence Loss (epoch 4) : 1.489043\n",
      "Single sentence Loss (epoch 4) : 0.849718\n",
      "Single sentence Loss (epoch 4) : 0.362828\n",
      "Single sentence Loss (epoch 4) : 1.007396\n",
      "Single sentence Loss (epoch 4) : 0.878468\n",
      "Single sentence Loss (epoch 4) : 1.108062\n",
      "Single sentence Loss (epoch 4) : 0.269587\n",
      "Single sentence Loss (epoch 4) : 1.973943\n",
      "Single sentence Loss (epoch 4) : 0.583772\n",
      "Single sentence Loss (epoch 4) : 0.493469\n",
      "Single sentence Loss (epoch 4) : 0.523963\n",
      "Single sentence Loss (epoch 4) : 0.027654\n",
      "Single sentence Loss (epoch 4) : 0.736453\n",
      "Single sentence Loss (epoch 4) : 0.919972\n",
      "Single sentence Loss (epoch 4) : 0.330154\n",
      "Single sentence Loss (epoch 4) : 0.186572\n",
      "Single sentence Loss (epoch 4) : 1.387198\n",
      "Single sentence Loss (epoch 4) : 0.721464\n",
      "Single sentence Loss (epoch 4) : 0.006474\n",
      "Single sentence Loss (epoch 4) : 0.458751\n",
      "Single sentence Loss (epoch 4) : 0.909366\n",
      "Single sentence Loss (epoch 4) : 0.193485\n",
      "Single sentence Loss (epoch 4) : 1.315484\n",
      "Single sentence Loss (epoch 4) : 0.247832\n",
      "Single sentence Loss (epoch 4) : 0.742139\n",
      "Single sentence Loss (epoch 4) : 0.257196\n",
      "Single sentence Loss (epoch 4) : 0.549279\n",
      "Single sentence Loss (epoch 4) : 0.394336\n",
      "Single sentence Loss (epoch 4) : 0.396793\n",
      "Single sentence Loss (epoch 4) : 0.064477\n",
      "Single sentence Loss (epoch 4) : 0.748622\n",
      "Single sentence Loss (epoch 4) : 0.808768\n",
      "Single sentence Loss (epoch 4) : 0.872391\n",
      "Loss (epoch 4) : 0.550387\n"
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = DecoderLSTM()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end-to-end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, input_sentence in enumerate(filtered_sentences):\n",
    "        output_sentence = piglatin_filtered_sentences[id]\n",
    "        loss = train(input_sentence, output_sentence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"LSTM\") \n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-KzySeXdkdP"
   },
   "source": [
    "## 1.4. Building Language Decoder MAP Inference\n",
    "\n",
    "We now define a method to perform inference with our decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "Fq1fkD-wdkdP",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def inference(input_sentence, encoder, decoder, decoderType=\"LSTM\", embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # Lengths of input\n",
    "    input_len = len(input_sentence)\n",
    "\n",
    "    # Initialize encoder & decoder \n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Initialize the output layer\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Encoder hidden / cell states \n",
    "    encoder_hidden = encoder.hidden\n",
    "    encoder_cell = encoder.cell\n",
    "    \n",
    "    # Initialize encoder hidden states (for Attention LSTM decoder and/or Transforemer decoder)\n",
    "    encoder_hidden_states = torch.zeros(max_length, encoder.hidden_dim)\n",
    "    # Initialize attention (for Attention LSTM decoder and/or Transforemer decoder)\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # Iterate over the input sequence \n",
    "    for ei in range(1, input_len):\n",
    "        # Get the current word index\n",
    "        word_idx = word2index[input_sentence[ei]]\n",
    "        # Convert to a 1-hot encoding\n",
    "        encoder_input = torch.Tensor(embeddings[word_idx])\n",
    "        # Run one step of the encoder\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "        # Save the encoder hidden states for future processing\n",
    "        encoder_hidden_states[ei,:] = encoder_hidden[0]\n",
    "\n",
    "    # Start decoding from <SOS> token\n",
    "    word_indx = word2index[\"<SOS>\"]\n",
    "    decoder_input = torch.FloatTensor(embeddings[word_indx]).cuda()\n",
    "    if decoderType == \"Transformer\":\n",
    "        decoder_inputs = decoder_input.unsqueeze(0)\n",
    "\n",
    "    # Initialize decoder hidden/cell state to the last encoder hidden/cell state\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "\n",
    "    decoded_words = []\n",
    "    index_list=[]\n",
    "\n",
    "    # Iterate up to the max_length of output\n",
    "    for i in range(max_length):\n",
    "        if decoderType == \"LSTM\": \n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "\n",
    "        if decoderType == \"AttentionLSTM\":\n",
    "            decoder_output, decoder_hidden, decoder_cell, attention_weights = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_hidden_states.cuda())\n",
    "            decoder_attentions[i] = attention_weights.detach().cpu()[0,:,0]\n",
    "\n",
    "        if decoderType == \"Transformer\":\n",
    "            decoder_output, encoder_decoder_attn, self_attn = decoder(decoder_inputs, decoder_hidden, decoder_cell, encoder_hidden_states.cuda())\n",
    "            decoder_attentions = encoder_decoder_attn\n",
    "            decoder_output = decoder_output[:,-1:,:]\n",
    "\n",
    "        # Get the highest probability word index\n",
    "        topv, topi = softmax(decoder_output[0]).topk(1)\n",
    "        index = topi.squeeze().item()\n",
    "        # Set it as the next decoder input\n",
    "        decoder_input = torch.FloatTensor(embeddings[index]).cuda()\n",
    "        if decoderType == \"Transformer\":\n",
    "            decoder_inputs = torch.cat([decoder_inputs, decoder_input.unsqueeze(0)], dim=0)\n",
    "        # If token is <EOS> then stop\n",
    "        if index == word2index[\"<EOS>\"]:\n",
    "            break\n",
    "        # otherwise append to the output index list\n",
    "        index_list.append(index)\n",
    "\n",
    "    # Convert word token indexes to the actual output string\n",
    "    word_list= \"\"\n",
    "    for i in range(len(index_list)):\n",
    "        word=vocabulary[index_list[i]]\n",
    "        word_list = word_list + \" \" + word\n",
    "\n",
    "    return word_list, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3E3scOrkxtHV",
    "outputId": "ad971456-d822-4acb-fbf6-e0196299ecf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: A very clean and well decorated empty bathroom\n",
      "Pig Latin:  away eryvay eanclay andway ilversay ecoratedday ecoratedday ecoratedday\n"
     ]
    }
   ],
   "source": [
    "# Lets test it \n",
    "sentence = \"A very clean and well decorated empty bathroom\" \n",
    "input_sentence = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] \n",
    "output_sentence, _ = inference(input_sentence, encoder, decoder, decoderType=\"LSTM\")\n",
    "\n",
    "print(\"English: \" + sentence)\n",
    "print(\"Pig Latin: \" + output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaZkwa6EdkdQ"
   },
   "source": [
    "## 1.5. Building Language Decoder Sampling Inference\n",
    "\n",
    "We now modify the inference method to sample from the distribution outputted by the LSTM rather than taking the most probable word.\n",
    "\n",
    "It might be useful to take a look at the output of your model and (depending on your implementation) modify it so that the outputs sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "id": "chHsbrX8dkdQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sampling_inference(input_sentence, encoder, decoder, decoderType=\"LSTM\", embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # Lengths of input\n",
    "    input_len = len(input_sentence)\n",
    "\n",
    "    # Initialize encoder & decoder \n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Initialize the output layer\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Encoder hidden / cell states \n",
    "    encoder_hidden = encoder.hidden\n",
    "    encoder_cell = encoder.cell\n",
    "    \n",
    "    # Initialize encoder hidden states (for Attention LSTM decoder and/or Transforemer decoder)\n",
    "    encoder_hidden_states = torch.zeros(max_length, encoder.hidden_dim)\n",
    "    # Initialize attention (for Attention LSTM decoder and/or Transforemer decoder)\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # Iterate over the input sequence \n",
    "    for ei in range(1, input_len):\n",
    "        # Get the current word index\n",
    "        word_idx = word2index[input_sentence[ei]]\n",
    "        # Convert to a 1-hot encoding\n",
    "        encoder_input = torch.Tensor(embeddings[word_idx])\n",
    "        # Run one step of the encoder\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "        # Save the encoder hidden states for future processing\n",
    "        encoder_hidden_states[ei,:] = encoder_hidden[0]\n",
    "\n",
    "    # Start decoding from <SOS> token\n",
    "    word_indx = word2index[\"<SOS>\"]\n",
    "    decoder_input = torch.FloatTensor(embeddings[word_indx]).cuda()\n",
    "    if decoderType == \"Transformer\":\n",
    "        decoder_inputs = decoder_input.unsqueeze(0)\n",
    "\n",
    "    # Initialize decoder hidden/cell state to the last encoder hidden/cell state\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "\n",
    "    decoded_words = []\n",
    "    index_list=[]\n",
    "\n",
    "    # Iterate up to the max_length of output\n",
    "    for i in range(max_length):\n",
    "        if decoderType == \"LSTM\": \n",
    "            decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "\n",
    "        if decoderType == \"AttentionLSTM\":\n",
    "            decoder_output, decoder_hidden, decoder_cell, attention_weights = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_hidden_states.cuda())\n",
    "            decoder_attentions[i] = attention_weights.detach().cpu()\n",
    "          \n",
    "        if decoderType == \"Transformer\":\n",
    "            decoder_output, encoder_decoder_attn, self_attn = decoder(decoder_inputs, decoder_hidden, decoder_cell, encoder_hidden_states.cuda())\n",
    "            decoder_attentions = encoder_decoder_attn\n",
    "            decoder_output = decoder_output[:,-1:,:]\n",
    "\n",
    "        # Get probability for each word\n",
    "        probs = np.exp(softmax(decoder_output[0]).cpu().detach().numpy().squeeze())\n",
    "        # Sample according to probability\n",
    "        sample_sum = probs[0]\n",
    "        random_sample = random()\n",
    "        index = 0\n",
    "        while sample_sum < random_sample:\n",
    "            index += 1\n",
    "            sample_sum += probs[index]\n",
    "\n",
    "        # Set it as the next decoder input\n",
    "        decoder_input = torch.FloatTensor(embeddings[index]).cuda()\n",
    "        if decoderType == \"Transformer\":\n",
    "            decoder_inputs = torch.cat([decoder_inputs, decoder_input.unsqueeze(0)], dim=0)\n",
    "        # If token is <EOS> then stop\n",
    "        if index == word2index[\"<EOS>\"]:\n",
    "            break\n",
    "        # otherwise append to the output index list\n",
    "        index_list.append(index)\n",
    "\n",
    "    # Convert word token indexes to the actual output string\n",
    "    word_list= \"\"\n",
    "    for i in range(len(index_list)):\n",
    "        word=vocabulary[index_list[i]]\n",
    "        word_list = word_list + \" \" + word\n",
    "\n",
    "    return word_list, decoder_attentions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6jeTIFOyfa7",
    "outputId": "b24f77d4-cc84-417b-d805-05468cd59086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: A very clean and well decorated empty bathroom\n",
      "Pig Latin:  away eryvay eanclay andway assglay ecoratedday ashionedfay esidentialray\n",
      "Pig Latin:  away eryvay eanclay andway emptyway iningday ecoratedday ecoratedday\n",
      "Pig Latin:  away eryvay eanclay eanclay eanclay assglay ousehay ashay\n",
      "Pig Latin:  away eryvay eanclay andway edsbay ecoratedday esktopday ecoratedday\n",
      "Pig Latin:  away eanclay eanclay andway omehay oldway ublicpay ebay\n"
     ]
    }
   ],
   "source": [
    "# Lets test it \n",
    "sentence = \"A very clean and well decorated empty bathroom\" \n",
    "input_sentence = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] \n",
    "\n",
    "print(\"English: \" + sentence)\n",
    "\n",
    "for i in range(5):\n",
    "    output_sentence, _ = sampling_inference(input_sentence, encoder, decoder, decoderType=\"LSTM\")\n",
    "    print(\"Pig Latin: \" + output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe3Aq_-VdkdV"
   },
   "source": [
    "## 1.6. Testing \n",
    "\n",
    "We must now define a method that allows us to do inference using the seq2seq architecture. We then run the 500 validation captions through this method, and ultimately compare the **reference** and **generated** sentences using our **BLEU** similarity score method defined above, to identify the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcxSh_RWdkdR",
    "outputId": "60b34b13-0dee-420a-a134-e11c590c3d10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distance between \n",
      "  \"Set of bananas hanging off of a banana tree.\" \n",
      "and\n",
      "  \" anymay ofway ananasbay anginghay omfray away eetray ofway away .\" \n",
      " is: 0.4132584091896901\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Two bunches of green bananas on banana trees.\" \n",
      "and\n",
      "  \" otway eengray eengray ananasbay iledpay onway eengray eaveslay .\" \n",
      " is: 0.5133450480401704\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Many calendars and bunches of bananas hanging on a wall.\" \n",
      "and\n",
      "  \" anymay andway andway otherway owersflay onway away ockclay owertay .\" \n",
      " is: 0.439291121189358\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Clusters of bananas and pictures hanging on a wall.\" \n",
      "and\n",
      "  \" ofway ananasbay andway anginghay anginghay onway away allway .\" \n",
      " is: 0.5341735956899847\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"two dogs that look to be fighting one another\" \n",
      "and\n",
      "  \" otway uffedstay atthay areway otay oneway anotherway anmay\" \n",
      " is: 0.4824015383731099\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLUE score (ArgMAX inference): 0.526813\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, sent in enumerate(val_sentences): \n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(sent.lower()) + [\"<EOS>\"] \n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "\n",
    "    output_sentence, _ = inference(input_good, encoder, decoder, decoderType=\"LSTM\")\n",
    "    target_sentence = piglatin_val_sentences[idx]\n",
    "    score = compute_bleu(target_sentence, output_sentence)\n",
    "    avg_score.append(score)\n",
    "\n",
    "    if idx < 5 :\n",
    "        print('BLEU score distance between \\n  \"' + sent + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "    \n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLUE score (ArgMAX inference): %f\" % (final_score)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANNWJRM8IpE1",
    "outputId": "88320016-ade2-4163-dbc6-4842613f9026"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distance between \n",
      "  \"Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\" \n",
      "and\n",
      "  \" aketay ofway anginghay anginghay omfray away oatcay ofway orangeway .\" \n",
      " is: 0.8408964152537145\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway unchesbay ofway eengray ananasbay onway ananabay eestray.\" \n",
      "and\n",
      "  \" otway eengray eengray ofway ananasbay onway eengray eaveslay .\" \n",
      " is: 0.537284965911771\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Anymay alendarscay andway unchesbay ofway ananasbay anginghay onway away allway.\" \n",
      "and\n",
      "  \" anymay andway anymay oloredcay andway itsay onway away ockclay .\" \n",
      " is: 0.439291121189358\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Ustersclay ofway ananasbay andway icturespay anginghay onway away allway.\" \n",
      "and\n",
      "  \" ofway andway owersflay anginghay outway onway away itewhay .\" \n",
      " is: 0.48078371183112106\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"otway ogsday atthay ooklay otay ebay ightingfay oneway anotherway\" \n",
      "and\n",
      "  \" otway idskay atthay , otay oneway anotherway enjoyingway\" \n",
      " is: 0.4824015383731099\n",
      "\n",
      "\n",
      "Average BLUE score (sampling inference): 0.472695\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, sent in enumerate(val_sentences): \n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(sent.lower()) + [\"<EOS>\"]\n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "\n",
    "    output_sentence, _ = sampling_inference(input_good, encoder, decoder, decoderType=\"LSTM\")\n",
    "    target_sentence = piglatin_val_sentences[idx]\n",
    "    score = compute_bleu(target_sentence, output_sentence)\n",
    "    avg_score.append(score)\n",
    "\n",
    "    if idx < 5 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLUE score (sampling inference): %f\" % (final_score)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tL3LKIZ7dkdQ"
   },
   "source": [
    "## 1.7. Experiment with Teacher Forcing\n",
    "\n",
    "Redo steps 1.3 and 1.6 with teacher_forcing_ratio = 0.9 and 0.8. Comment on the results, speed of convergence and the quality of results. Note that in most real scenarious the teacher forcing is actually annealed; starting with teacher forcing = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "DX-D_PI7dkdV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b397f3cc-7598-4781-ffc3-9d044b3b45dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training end-to-end network ......\n",
      "Single sentence Loss (epoch 0) : 3.081385\n",
      "Single sentence Loss (epoch 0) : 2.782010\n",
      "Single sentence Loss (epoch 0) : 3.109362\n",
      "Single sentence Loss (epoch 0) : 3.837219\n",
      "Single sentence Loss (epoch 0) : 3.319787\n",
      "Single sentence Loss (epoch 0) : 2.072450\n",
      "Single sentence Loss (epoch 0) : 3.969132\n",
      "Single sentence Loss (epoch 0) : 4.489776\n",
      "Single sentence Loss (epoch 0) : 4.316211\n",
      "Single sentence Loss (epoch 0) : 2.667988\n",
      "Single sentence Loss (epoch 0) : 2.995584\n",
      "Single sentence Loss (epoch 0) : 3.318458\n",
      "Single sentence Loss (epoch 0) : 4.215368\n",
      "Single sentence Loss (epoch 0) : 1.955214\n",
      "Single sentence Loss (epoch 0) : 4.384216\n",
      "Single sentence Loss (epoch 0) : 3.075862\n",
      "Single sentence Loss (epoch 0) : 3.214308\n",
      "Single sentence Loss (epoch 0) : 4.014882\n",
      "Single sentence Loss (epoch 0) : 2.664733\n",
      "Single sentence Loss (epoch 0) : 5.094522\n",
      "Single sentence Loss (epoch 0) : 3.199577\n",
      "Single sentence Loss (epoch 0) : 2.778389\n",
      "Single sentence Loss (epoch 0) : 2.789619\n",
      "Single sentence Loss (epoch 0) : 4.291264\n",
      "Single sentence Loss (epoch 0) : 3.918023\n",
      "Single sentence Loss (epoch 0) : 1.623132\n",
      "Single sentence Loss (epoch 0) : 3.205363\n",
      "Single sentence Loss (epoch 0) : 2.379870\n",
      "Single sentence Loss (epoch 0) : 1.876130\n",
      "Single sentence Loss (epoch 0) : 2.864661\n",
      "Single sentence Loss (epoch 0) : 1.788033\n",
      "Single sentence Loss (epoch 0) : 2.507968\n",
      "Single sentence Loss (epoch 0) : 2.193124\n",
      "Single sentence Loss (epoch 0) : 2.861921\n",
      "Single sentence Loss (epoch 0) : 2.478849\n",
      "Single sentence Loss (epoch 0) : 2.178382\n",
      "Single sentence Loss (epoch 0) : 1.236485\n",
      "Single sentence Loss (epoch 0) : 4.167405\n",
      "Single sentence Loss (epoch 0) : 2.563102\n",
      "Single sentence Loss (epoch 0) : 4.299306\n",
      "Loss (epoch 0) : 2.856036\n",
      "Single sentence Loss (epoch 1) : 2.082301\n",
      "Single sentence Loss (epoch 1) : 0.914655\n",
      "Single sentence Loss (epoch 1) : 2.537919\n",
      "Single sentence Loss (epoch 1) : 4.594728\n",
      "Single sentence Loss (epoch 1) : 1.709254\n",
      "Single sentence Loss (epoch 1) : 0.997946\n",
      "Single sentence Loss (epoch 1) : 4.613029\n",
      "Single sentence Loss (epoch 1) : 3.549969\n",
      "Single sentence Loss (epoch 1) : 3.157510\n",
      "Single sentence Loss (epoch 1) : 1.500384\n",
      "Single sentence Loss (epoch 1) : 2.405655\n",
      "Single sentence Loss (epoch 1) : 2.310489\n",
      "Single sentence Loss (epoch 1) : 3.666816\n",
      "Single sentence Loss (epoch 1) : 1.539154\n",
      "Single sentence Loss (epoch 1) : 3.683569\n",
      "Single sentence Loss (epoch 1) : 4.721307\n",
      "Single sentence Loss (epoch 1) : 2.891081\n",
      "Single sentence Loss (epoch 1) : 3.919920\n",
      "Single sentence Loss (epoch 1) : 1.125692\n",
      "Single sentence Loss (epoch 1) : 2.033235\n",
      "Single sentence Loss (epoch 1) : 2.226233\n",
      "Single sentence Loss (epoch 1) : 2.415810\n",
      "Single sentence Loss (epoch 1) : 1.478439\n",
      "Single sentence Loss (epoch 1) : 3.297996\n",
      "Single sentence Loss (epoch 1) : 2.921696\n",
      "Single sentence Loss (epoch 1) : 0.683383\n",
      "Single sentence Loss (epoch 1) : 1.948774\n",
      "Single sentence Loss (epoch 1) : 1.752816\n",
      "Single sentence Loss (epoch 1) : 1.315311\n",
      "Single sentence Loss (epoch 1) : 2.327497\n",
      "Single sentence Loss (epoch 1) : 1.139243\n",
      "Single sentence Loss (epoch 1) : 2.038645\n",
      "Single sentence Loss (epoch 1) : 1.127687\n",
      "Single sentence Loss (epoch 1) : 1.491776\n",
      "Single sentence Loss (epoch 1) : 1.495669\n",
      "Single sentence Loss (epoch 1) : 1.632943\n",
      "Single sentence Loss (epoch 1) : 0.623362\n",
      "Single sentence Loss (epoch 1) : 2.336226\n",
      "Single sentence Loss (epoch 1) : 1.609910\n",
      "Single sentence Loss (epoch 1) : 2.781646\n",
      "Loss (epoch 1) : 1.922419\n",
      "Single sentence Loss (epoch 2) : 1.194281\n",
      "Single sentence Loss (epoch 2) : 0.400743\n",
      "Single sentence Loss (epoch 2) : 1.189824\n",
      "Single sentence Loss (epoch 2) : 2.538441\n",
      "Single sentence Loss (epoch 2) : 0.980917\n",
      "Single sentence Loss (epoch 2) : 0.680129\n",
      "Single sentence Loss (epoch 2) : 2.275592\n",
      "Single sentence Loss (epoch 2) : 2.743973\n",
      "Single sentence Loss (epoch 2) : 3.373494\n",
      "Single sentence Loss (epoch 2) : 1.033729\n",
      "Single sentence Loss (epoch 2) : 1.795148\n",
      "Single sentence Loss (epoch 2) : 1.716940\n",
      "Single sentence Loss (epoch 2) : 2.519114\n",
      "Single sentence Loss (epoch 2) : 0.928691\n",
      "Single sentence Loss (epoch 2) : 2.559147\n",
      "Single sentence Loss (epoch 2) : 1.549785\n",
      "Single sentence Loss (epoch 2) : 1.861092\n",
      "Single sentence Loss (epoch 2) : 1.738478\n",
      "Single sentence Loss (epoch 2) : 0.296078\n",
      "Single sentence Loss (epoch 2) : 1.431482\n",
      "Single sentence Loss (epoch 2) : 1.410540\n",
      "Single sentence Loss (epoch 2) : 1.536176\n",
      "Single sentence Loss (epoch 2) : 0.627780\n",
      "Single sentence Loss (epoch 2) : 2.567175\n",
      "Single sentence Loss (epoch 2) : 1.941525\n",
      "Single sentence Loss (epoch 2) : 0.406429\n",
      "Single sentence Loss (epoch 2) : 1.211594\n",
      "Single sentence Loss (epoch 2) : 1.068644\n",
      "Single sentence Loss (epoch 2) : 0.507903\n",
      "Single sentence Loss (epoch 2) : 1.620383\n",
      "Single sentence Loss (epoch 2) : 0.660213\n",
      "Single sentence Loss (epoch 2) : 0.717486\n",
      "Single sentence Loss (epoch 2) : 0.436656\n",
      "Single sentence Loss (epoch 2) : 1.160530\n",
      "Single sentence Loss (epoch 2) : 1.156611\n",
      "Single sentence Loss (epoch 2) : 1.001239\n",
      "Single sentence Loss (epoch 2) : 0.256721\n",
      "Single sentence Loss (epoch 2) : 1.356163\n",
      "Single sentence Loss (epoch 2) : 1.498843\n",
      "Single sentence Loss (epoch 2) : 1.716448\n",
      "Loss (epoch 2) : 1.273508\n",
      "Single sentence Loss (epoch 3) : 0.540274\n",
      "Single sentence Loss (epoch 3) : 0.237409\n",
      "Single sentence Loss (epoch 3) : 0.559751\n",
      "Single sentence Loss (epoch 3) : 1.375355\n",
      "Single sentence Loss (epoch 3) : 0.278983\n",
      "Single sentence Loss (epoch 3) : 0.353057\n",
      "Single sentence Loss (epoch 3) : 1.484204\n",
      "Single sentence Loss (epoch 3) : 2.984639\n",
      "Single sentence Loss (epoch 3) : 1.605507\n",
      "Single sentence Loss (epoch 3) : 0.669024\n",
      "Single sentence Loss (epoch 3) : 1.722453\n",
      "Single sentence Loss (epoch 3) : 1.624250\n",
      "Single sentence Loss (epoch 3) : 1.758002\n",
      "Single sentence Loss (epoch 3) : 0.351277\n",
      "Single sentence Loss (epoch 3) : 2.027126\n",
      "Single sentence Loss (epoch 3) : 1.042869\n",
      "Single sentence Loss (epoch 3) : 1.009506\n",
      "Single sentence Loss (epoch 3) : 1.199976\n",
      "Single sentence Loss (epoch 3) : 0.238108\n",
      "Single sentence Loss (epoch 3) : 0.964321\n",
      "Single sentence Loss (epoch 3) : 1.003993\n",
      "Single sentence Loss (epoch 3) : 1.010325\n",
      "Single sentence Loss (epoch 3) : 0.309008\n",
      "Single sentence Loss (epoch 3) : 1.920357\n",
      "Single sentence Loss (epoch 3) : 1.449612\n",
      "Single sentence Loss (epoch 3) : 0.088884\n",
      "Single sentence Loss (epoch 3) : 0.765669\n",
      "Single sentence Loss (epoch 3) : 0.714230\n",
      "Single sentence Loss (epoch 3) : 0.703585\n",
      "Single sentence Loss (epoch 3) : 1.559637\n",
      "Single sentence Loss (epoch 3) : 0.518676\n",
      "Single sentence Loss (epoch 3) : 0.793271\n",
      "Single sentence Loss (epoch 3) : 0.312500\n",
      "Single sentence Loss (epoch 3) : 1.141116\n",
      "Single sentence Loss (epoch 3) : 0.942544\n",
      "Single sentence Loss (epoch 3) : 0.356145\n",
      "Single sentence Loss (epoch 3) : 0.067113\n",
      "Single sentence Loss (epoch 3) : 0.662258\n",
      "Single sentence Loss (epoch 3) : 0.883100\n",
      "Single sentence Loss (epoch 3) : 1.290826\n",
      "Loss (epoch 3) : 0.850958\n",
      "Single sentence Loss (epoch 4) : 0.428270\n",
      "Single sentence Loss (epoch 4) : 0.055230\n",
      "Single sentence Loss (epoch 4) : 0.242708\n",
      "Single sentence Loss (epoch 4) : 1.014690\n",
      "Single sentence Loss (epoch 4) : 0.175078\n",
      "Single sentence Loss (epoch 4) : 0.098434\n",
      "Single sentence Loss (epoch 4) : 1.169905\n",
      "Single sentence Loss (epoch 4) : 1.591021\n",
      "Single sentence Loss (epoch 4) : 0.950667\n",
      "Single sentence Loss (epoch 4) : 0.557736\n",
      "Single sentence Loss (epoch 4) : 1.598866\n",
      "Single sentence Loss (epoch 4) : 0.717214\n",
      "Single sentence Loss (epoch 4) : 1.050339\n",
      "Single sentence Loss (epoch 4) : 0.114562\n",
      "Single sentence Loss (epoch 4) : 1.717382\n",
      "Single sentence Loss (epoch 4) : 0.627521\n",
      "Single sentence Loss (epoch 4) : 0.781416\n",
      "Single sentence Loss (epoch 4) : 0.721426\n",
      "Single sentence Loss (epoch 4) : 0.057559\n",
      "Single sentence Loss (epoch 4) : 0.604949\n",
      "Single sentence Loss (epoch 4) : 0.597719\n",
      "Single sentence Loss (epoch 4) : 0.604239\n",
      "Single sentence Loss (epoch 4) : 0.138795\n",
      "Single sentence Loss (epoch 4) : 2.105244\n",
      "Single sentence Loss (epoch 4) : 0.836991\n",
      "Single sentence Loss (epoch 4) : 0.021290\n",
      "Single sentence Loss (epoch 4) : 0.246405\n",
      "Single sentence Loss (epoch 4) : 0.564083\n",
      "Single sentence Loss (epoch 4) : 0.187906\n",
      "Single sentence Loss (epoch 4) : 1.098184\n",
      "Single sentence Loss (epoch 4) : 0.170160\n",
      "Single sentence Loss (epoch 4) : 0.452613\n",
      "Single sentence Loss (epoch 4) : 0.132913\n",
      "Single sentence Loss (epoch 4) : 0.650111\n",
      "Single sentence Loss (epoch 4) : 0.553315\n",
      "Single sentence Loss (epoch 4) : 0.204659\n",
      "Single sentence Loss (epoch 4) : 0.022158\n",
      "Single sentence Loss (epoch 4) : 0.342841\n",
      "Single sentence Loss (epoch 4) : 0.683668\n",
      "Single sentence Loss (epoch 4) : 0.978419\n",
      "Loss (epoch 4) : 0.567177\n"
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = DecoderLSTM()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end-to-end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, input_sentence in enumerate(filtered_sentences):\n",
    "        output_sentence = piglatin_filtered_sentences[id]\n",
    "        loss = train(input_sentence, output_sentence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio = 0.9, decoderType=\"LSTM\") \n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tI2hmhQqMWMA",
    "outputId": "af8534ed-a647-4f3b-b56d-602394977d84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLUE score (ArgMAX inference): 0.387931\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, sent in enumerate(val_sentences): \n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(sent.lower()) + [\"<EOS>\"]\n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "\n",
    "    output_sentence, _ = inference(input_good, encoder, decoder, decoderType=\"LSTM\")\n",
    "    target_sentence = piglatin_val_sentences[idx]\n",
    "    score = compute_bleu(target_sentence, output_sentence)\n",
    "    avg_score.append(score)\n",
    "\n",
    "    if idx < 5 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "\n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLUE score (ArgMAX inference): %f\" % (final_score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3wDN2Z1dkdX"
   },
   "source": [
    "## 1.8. Encoding as Generic Feature Representation\n",
    "\n",
    "We now use the final hidden state of our encoder, to identify the nearest neighbor amongst the training sentences for each sentence in our validation data.\n",
    "\n",
    "It would be effective to first define a method that would generate all of the hidden states and store these hidden states **on the CPU**, and then loop over the generated hidden states to identify/output the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "id": "IxFmDA_YdkdY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def final_encoder_hidden(input_sentence, encoder, embeddings=one_hot_embeddings):\n",
    "    # Lengths of input\n",
    "    input_len = len(input_sentence)\n",
    "\n",
    "    # Initialize encoder \n",
    "    encoder.cuda()\n",
    "    encoder.eval()\n",
    "\n",
    "    # Encoder hidden / cell states \n",
    "    encoder_hidden = encoder.hidden\n",
    "    encoder_cell = encoder.cell\n",
    "    \n",
    "    # Iterate over the input sequence \n",
    "    for ei in range(0, input_len):\n",
    "        # Get the current word index\n",
    "        word_idx = word2index[input_sentence[ei]]\n",
    "        # Convert to a 1-hot encoding\n",
    "        encoder_input = torch.Tensor(embeddings[word_idx])\n",
    "        # Run one step of the encoder\n",
    "        encoder_output, encoder_hidden, encoder_cell = encoder(encoder_input, encoder_hidden, encoder_cell)\n",
    "\n",
    "    return encoder_hidden.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "9TS11T9MR_30"
   },
   "outputs": [],
   "source": [
    "train_hiddens = np.zeros([1000,encoder.hidden_dim])\n",
    "val_hiddens = np.zeros([len(val_sentences),encoder.hidden_dim])\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    sentence = train_sentences[i]\n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"]\n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "    train_hiddens[i,:] = final_encoder_hidden(input_good, encoder)\n",
    "\n",
    "for i in range(0, len(val_sentences)):\n",
    "    sentence = val_sentences[i]\n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"]\n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "    val_hiddens[i,:] = final_encoder_hidden(input_good, encoder)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RpZtVzHpdkdY",
    "outputId": "1452ec12-59cc-4bb9-f0e9-ef8caf96d082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of bananas hanging off of a banana tree. || The dishes are on plates behind a glass display.\n",
      "Two bunches of green bananas on banana trees. || Two sliced of toasted angel food cake sitting on a white plate.\n",
      "Many calendars and bunches of bananas hanging on a wall. || Many different empty cans and bottles on the kitchen counter. \n",
      "Clusters of bananas and pictures hanging on a wall. || a banana and other foods on a wooden table\n",
      "two dogs that look to be fighting one another || two people jumping dirt bikes near each other\n",
      "Two dogs fighting with one on his back on the ground || Two women walking through a park followed by a small dog\n",
      "Bunches of green bananas hanging down from trees. || Six Green Peppers and a knife on a cutting board.\n",
      "Two dogs have a playful fight with one another. || Two women prepare a meal in a large kitchen.\n",
      "Banana trees with large  hanging bunches of bananas. || Six Green Peppers and a knife on a cutting board.\n",
      "There are some green bananas hanging in bunches || There are different views of a man holding a remote \n"
     ]
    }
   ],
   "source": [
    "for i,val_hidden in enumerate(val_hiddens[:10]):\n",
    "    closest_idx = min(range(len(train_hiddens)), key=lambda i: np.linalg.norm(train_hiddens[i] - val_hidden))\n",
    "    print(val_sentences[i], \"||\", train_sentences[closest_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFpfb1oi0efe"
   },
   "source": [
    "# Part 2: Attention LSTM Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx2bELJh03hq"
   },
   "source": [
    "## 2.1. Implementing Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f2adPSOL09Hj"
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_network = nn.Sequential(\n",
    "                                    nn.Linear(hidden_size*2, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, 1)\n",
    "                                 )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the additive attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x 1 x seq_len)\n",
    "\n",
    "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = queries.shape[0] \n",
    "        expanded_queries = queries.unsqueeze(1).expand_as(keys)  \n",
    "        concat_inputs = torch.cat([expanded_queries, keys], 2) \n",
    "        unnormalized_attention = self.attention_network(concat_inputs) \n",
    "        attention_weights = self.softmax(unnormalized_attention)  \n",
    "        context = torch.bmm(attention_weights.transpose(1,2),values.unsqueeze(0))\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOPm_dvX1TrZ"
   },
   "source": [
    "## 2.2. Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SY7vHNT_1oug"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        self.dropout_p = 0.1\n",
    "        self.linear_input = nn.Linear(wordEncodingSize, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.hidden_dim*2, self.hidden_dim)\n",
    "        self.attention = AdditiveAttention(hidden_size=self.hidden_dim)\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.cell = self.init_cell()\n",
    "\n",
    "    def init_hidden(self):    \n",
    "        return torch.randn(1,1, self.hidden_dim).cuda()\n",
    "\n",
    "    def init_cell(self):\n",
    "        return torch.randn(1,1, self.hidden_dim).cuda()\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell, encoder_hidden_states):\n",
    "        embed = self.dropout(self.linear_input(input_sentence.view(1,-1)))\n",
    "        context, attention_weights = self.attention(embed, encoder_hidden_states.unsqueeze(0), encoder_hidden_states) \n",
    "        embed_and_context = torch.cat([embed.unsqueeze(0),context], 2) \n",
    "        output, (hidden, cell) = self.lstm(embed_and_context, (hidden, cell))\n",
    "        output = self.linear(output)\n",
    "\n",
    "        return output, hidden , cell, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db7mTRflmQQu"
   },
   "source": [
    "## 2.3. Training Attention Decoder\n",
    "\n",
    "Note that you will need to modify the train() procedure for Part 1 to handles the AttentionLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEYM3U0VmdHp",
    "outputId": "0716d211-d32a-4c87-c71a-bfab2a54cbea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training end to end network ......\n",
      "Single sentence Loss (epoch 0) : 2.991728\n",
      "Single sentence Loss (epoch 0) : 2.331858\n",
      "Single sentence Loss (epoch 0) : 3.105178\n",
      "Single sentence Loss (epoch 0) : 3.488160\n",
      "Single sentence Loss (epoch 0) : 2.679361\n",
      "Single sentence Loss (epoch 0) : 2.117229\n",
      "Single sentence Loss (epoch 0) : 3.876295\n",
      "Single sentence Loss (epoch 0) : 4.360755\n",
      "Single sentence Loss (epoch 0) : 3.991102\n",
      "Single sentence Loss (epoch 0) : 2.660978\n",
      "Single sentence Loss (epoch 0) : 2.613211\n",
      "Single sentence Loss (epoch 0) : 2.999155\n",
      "Single sentence Loss (epoch 0) : 4.320385\n",
      "Single sentence Loss (epoch 0) : 1.720852\n",
      "Single sentence Loss (epoch 0) : 4.003938\n",
      "Single sentence Loss (epoch 0) : 2.870272\n",
      "Single sentence Loss (epoch 0) : 3.231789\n",
      "Single sentence Loss (epoch 0) : 2.731076\n",
      "Single sentence Loss (epoch 0) : 2.267179\n",
      "Single sentence Loss (epoch 0) : 5.233195\n",
      "Single sentence Loss (epoch 0) : 2.635818\n",
      "Single sentence Loss (epoch 0) : 2.313759\n",
      "Single sentence Loss (epoch 0) : 2.277937\n",
      "Single sentence Loss (epoch 0) : 4.301845\n",
      "Single sentence Loss (epoch 0) : 3.969668\n",
      "Single sentence Loss (epoch 0) : 1.140856\n",
      "Single sentence Loss (epoch 0) : 2.660917\n",
      "Single sentence Loss (epoch 0) : 2.187973\n",
      "Single sentence Loss (epoch 0) : 1.869904\n",
      "Single sentence Loss (epoch 0) : 2.706650\n",
      "Single sentence Loss (epoch 0) : 1.090014\n",
      "Single sentence Loss (epoch 0) : 2.204340\n",
      "Single sentence Loss (epoch 0) : 2.027265\n",
      "Single sentence Loss (epoch 0) : 1.569659\n",
      "Single sentence Loss (epoch 0) : 2.225997\n",
      "Single sentence Loss (epoch 0) : 1.867053\n",
      "Single sentence Loss (epoch 0) : 0.894408\n",
      "Single sentence Loss (epoch 0) : 2.437832\n",
      "Single sentence Loss (epoch 0) : 2.244039\n",
      "Single sentence Loss (epoch 0) : 3.904626\n",
      "Loss (epoch 0) : 2.521278\n",
      "Single sentence Loss (epoch 1) : 1.795878\n",
      "Single sentence Loss (epoch 1) : 0.769776\n",
      "Single sentence Loss (epoch 1) : 1.609874\n",
      "Single sentence Loss (epoch 1) : 2.596002\n",
      "Single sentence Loss (epoch 1) : 1.468113\n",
      "Single sentence Loss (epoch 1) : 1.005728\n",
      "Single sentence Loss (epoch 1) : 2.620800\n",
      "Single sentence Loss (epoch 1) : 3.458142\n",
      "Single sentence Loss (epoch 1) : 2.670767\n",
      "Single sentence Loss (epoch 1) : 1.542693\n",
      "Single sentence Loss (epoch 1) : 2.028566\n",
      "Single sentence Loss (epoch 1) : 2.415498\n",
      "Single sentence Loss (epoch 1) : 3.513508\n",
      "Single sentence Loss (epoch 1) : 1.308810\n",
      "Single sentence Loss (epoch 1) : 3.821986\n",
      "Single sentence Loss (epoch 1) : 2.115168\n",
      "Single sentence Loss (epoch 1) : 2.497671\n",
      "Single sentence Loss (epoch 1) : 2.663432\n",
      "Single sentence Loss (epoch 1) : 0.670927\n",
      "Single sentence Loss (epoch 1) : 2.093492\n",
      "Single sentence Loss (epoch 1) : 1.931498\n",
      "Single sentence Loss (epoch 1) : 1.610673\n",
      "Single sentence Loss (epoch 1) : 1.260454\n",
      "Single sentence Loss (epoch 1) : 3.240276\n",
      "Single sentence Loss (epoch 1) : 2.950398\n",
      "Single sentence Loss (epoch 1) : 0.695697\n",
      "Single sentence Loss (epoch 1) : 1.681653\n",
      "Single sentence Loss (epoch 1) : 1.843703\n",
      "Single sentence Loss (epoch 1) : 1.016903\n",
      "Single sentence Loss (epoch 1) : 2.347575\n",
      "Single sentence Loss (epoch 1) : 0.516653\n",
      "Single sentence Loss (epoch 1) : 0.981498\n",
      "Single sentence Loss (epoch 1) : 0.836518\n",
      "Single sentence Loss (epoch 1) : 1.266603\n",
      "Single sentence Loss (epoch 1) : 1.309355\n",
      "Single sentence Loss (epoch 1) : 1.365718\n",
      "Single sentence Loss (epoch 1) : 0.506039\n",
      "Single sentence Loss (epoch 1) : 1.722813\n",
      "Single sentence Loss (epoch 1) : 1.421242\n",
      "Single sentence Loss (epoch 1) : 2.689551\n",
      "Loss (epoch 1) : 1.654783\n",
      "Single sentence Loss (epoch 2) : 1.354741\n",
      "Single sentence Loss (epoch 2) : 0.325358\n",
      "Single sentence Loss (epoch 2) : 1.127830\n",
      "Single sentence Loss (epoch 2) : 1.962201\n",
      "Single sentence Loss (epoch 2) : 0.885861\n",
      "Single sentence Loss (epoch 2) : 0.438768\n",
      "Single sentence Loss (epoch 2) : 2.138917\n",
      "Single sentence Loss (epoch 2) : 2.711877\n",
      "Single sentence Loss (epoch 2) : 1.737077\n",
      "Single sentence Loss (epoch 2) : 0.989293\n",
      "Single sentence Loss (epoch 2) : 1.562038\n",
      "Single sentence Loss (epoch 2) : 1.846729\n",
      "Single sentence Loss (epoch 2) : 2.670454\n",
      "Single sentence Loss (epoch 2) : 0.628591\n",
      "Single sentence Loss (epoch 2) : 2.073899\n",
      "Single sentence Loss (epoch 2) : 1.361652\n",
      "Single sentence Loss (epoch 2) : 1.883121\n",
      "Single sentence Loss (epoch 2) : 1.653299\n",
      "Single sentence Loss (epoch 2) : 0.308042\n",
      "Single sentence Loss (epoch 2) : 1.368521\n",
      "Single sentence Loss (epoch 2) : 1.639665\n",
      "Single sentence Loss (epoch 2) : 1.364963\n",
      "Single sentence Loss (epoch 2) : 0.423546\n",
      "Single sentence Loss (epoch 2) : 2.067570\n",
      "Single sentence Loss (epoch 2) : 1.916323\n",
      "Single sentence Loss (epoch 2) : 0.199556\n",
      "Single sentence Loss (epoch 2) : 0.900918\n",
      "Single sentence Loss (epoch 2) : 1.416097\n",
      "Single sentence Loss (epoch 2) : 0.375614\n",
      "Single sentence Loss (epoch 2) : 1.162565\n",
      "Single sentence Loss (epoch 2) : 0.330262\n",
      "Single sentence Loss (epoch 2) : 0.913198\n",
      "Single sentence Loss (epoch 2) : 0.352606\n",
      "Single sentence Loss (epoch 2) : 1.176024\n",
      "Single sentence Loss (epoch 2) : 0.763528\n",
      "Single sentence Loss (epoch 2) : 0.911588\n",
      "Single sentence Loss (epoch 2) : 0.350092\n",
      "Single sentence Loss (epoch 2) : 1.170161\n",
      "Single sentence Loss (epoch 2) : 1.160983\n",
      "Single sentence Loss (epoch 2) : 1.890284\n",
      "Loss (epoch 2) : 1.089829\n",
      "Single sentence Loss (epoch 3) : 0.732129\n",
      "Single sentence Loss (epoch 3) : 0.068557\n",
      "Single sentence Loss (epoch 3) : 0.503997\n",
      "Single sentence Loss (epoch 3) : 1.028793\n",
      "Single sentence Loss (epoch 3) : 0.336555\n",
      "Single sentence Loss (epoch 3) : 0.252493\n",
      "Single sentence Loss (epoch 3) : 1.648144\n",
      "Single sentence Loss (epoch 3) : 2.042519\n",
      "Single sentence Loss (epoch 3) : 1.332352\n",
      "Single sentence Loss (epoch 3) : 0.690379\n",
      "Single sentence Loss (epoch 3) : 1.143000\n",
      "Single sentence Loss (epoch 3) : 0.938000\n",
      "Single sentence Loss (epoch 3) : 1.807342\n",
      "Single sentence Loss (epoch 3) : 0.256641\n",
      "Single sentence Loss (epoch 3) : 1.606736\n",
      "Single sentence Loss (epoch 3) : 0.656646\n",
      "Single sentence Loss (epoch 3) : 1.151686\n",
      "Single sentence Loss (epoch 3) : 0.647890\n",
      "Single sentence Loss (epoch 3) : 0.103758\n",
      "Single sentence Loss (epoch 3) : 0.683979\n",
      "Single sentence Loss (epoch 3) : 1.122893\n",
      "Single sentence Loss (epoch 3) : 0.889583\n",
      "Single sentence Loss (epoch 3) : 0.176962\n",
      "Single sentence Loss (epoch 3) : 1.481370\n",
      "Single sentence Loss (epoch 3) : 1.196755\n",
      "Single sentence Loss (epoch 3) : 0.030952\n",
      "Single sentence Loss (epoch 3) : 0.317643\n",
      "Single sentence Loss (epoch 3) : 0.931138\n",
      "Single sentence Loss (epoch 3) : 0.127599\n",
      "Single sentence Loss (epoch 3) : 1.150626\n",
      "Single sentence Loss (epoch 3) : 0.189206\n",
      "Single sentence Loss (epoch 3) : 0.828455\n",
      "Single sentence Loss (epoch 3) : 0.208260\n",
      "Single sentence Loss (epoch 3) : 0.781934\n",
      "Single sentence Loss (epoch 3) : 0.749158\n",
      "Single sentence Loss (epoch 3) : 0.593632\n",
      "Single sentence Loss (epoch 3) : 0.026275\n",
      "Single sentence Loss (epoch 3) : 0.706772\n",
      "Single sentence Loss (epoch 3) : 0.559904\n",
      "Single sentence Loss (epoch 3) : 1.435626\n",
      "Loss (epoch 3) : 0.680735\n",
      "Single sentence Loss (epoch 4) : 0.481978\n",
      "Single sentence Loss (epoch 4) : 0.010469\n",
      "Single sentence Loss (epoch 4) : 0.116032\n",
      "Single sentence Loss (epoch 4) : 0.838581\n",
      "Single sentence Loss (epoch 4) : 0.154079\n",
      "Single sentence Loss (epoch 4) : 0.022331\n",
      "Single sentence Loss (epoch 4) : 0.564634\n",
      "Single sentence Loss (epoch 4) : 1.692714\n",
      "Single sentence Loss (epoch 4) : 0.663946\n",
      "Single sentence Loss (epoch 4) : 0.290502\n",
      "Single sentence Loss (epoch 4) : 1.001956\n",
      "Single sentence Loss (epoch 4) : 0.424241\n",
      "Single sentence Loss (epoch 4) : 0.905686\n",
      "Single sentence Loss (epoch 4) : 0.028558\n",
      "Single sentence Loss (epoch 4) : 0.999175\n",
      "Single sentence Loss (epoch 4) : 0.319492\n",
      "Single sentence Loss (epoch 4) : 0.593236\n",
      "Single sentence Loss (epoch 4) : 0.302602\n",
      "Single sentence Loss (epoch 4) : 0.009462\n",
      "Single sentence Loss (epoch 4) : 0.522893\n",
      "Single sentence Loss (epoch 4) : 0.581333\n",
      "Single sentence Loss (epoch 4) : 0.609456\n",
      "Single sentence Loss (epoch 4) : 0.119735\n",
      "Single sentence Loss (epoch 4) : 0.647152\n",
      "Single sentence Loss (epoch 4) : 0.655154\n",
      "Single sentence Loss (epoch 4) : 0.009606\n",
      "Single sentence Loss (epoch 4) : 0.054566\n",
      "Single sentence Loss (epoch 4) : 0.603769\n",
      "Single sentence Loss (epoch 4) : 0.037535\n",
      "Single sentence Loss (epoch 4) : 0.624982\n",
      "Single sentence Loss (epoch 4) : 0.060112\n",
      "Single sentence Loss (epoch 4) : 0.509246\n",
      "Single sentence Loss (epoch 4) : 0.057498\n",
      "Single sentence Loss (epoch 4) : 0.390376\n",
      "Single sentence Loss (epoch 4) : 0.260348\n",
      "Single sentence Loss (epoch 4) : 0.104450\n",
      "Single sentence Loss (epoch 4) : 0.006257\n",
      "Single sentence Loss (epoch 4) : 0.291112\n",
      "Single sentence Loss (epoch 4) : 0.058682\n",
      "Single sentence Loss (epoch 4) : 0.492497\n",
      "Loss (epoch 4) : 0.367482\n"
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = AttentionDecoder()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end to end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, sentence in enumerate(filtered_sentences):\n",
    "        target_variable = piglatin_filtered_sentences[id]\n",
    "        loss = train(sentence, target_variable, encoder, decoder,encoder_optimizer,decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"AttentionLSTM\")\n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFTuFon2m7f9"
   },
   "source": [
    "## 2.4. Testing Attention Decoder\n",
    "Note that you will need to modify the inference() procedure for Part 1 to handle Attention LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yx3flyVdnGCg",
    "outputId": "7260efa7-cc7b-4f43-ebed-0a1c0416d657"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distance between \n",
      "  \"Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\" \n",
      "and\n",
      "  \" ultiplemay ofway ananasbay anginghay upway ofway away ananabay anginghay .\" \n",
      " is: 0.5280972216470737\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway unchesbay ofway eengray ananasbay onway ananabay eestray.\" \n",
      "and\n",
      "  \" otway unchesbay ofway eengray ananasbay onway isplayday .\" \n",
      " is: 0.6240195441936914\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Anymay alendarscay andway unchesbay ofway ananasbay anginghay onway away allway.\" \n",
      "and\n",
      "  \" anymay ananasbay andway ananasbay anginghay onway away allway indowway .\" \n",
      " is: 0.39974977214602886\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Ustersclay ofway ananasbay andway icturespay anginghay onway away allway.\" \n",
      "and\n",
      "  \" ofway ananasbay andway anginghay anginghay onway away allway .\" \n",
      " is: 0.5341735956899847\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"otway ogsday atthay ooklay otay ebay ightingfay oneway anotherway\" \n",
      "and\n",
      "  \" otway ogsday atthay ooklay otay ooklay ikelay oneway\" \n",
      " is: 0.47750342648354643\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway ogsday ightingfay ithway oneway onway ishay ackbay onway ethay oundgray\" \n",
      "and\n",
      "  \" otway ogsday ithway oneway onway ethay ackbay onway ethay ackbay\" \n",
      " is: 0.5224081268759072\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Unchesbay ofway eengray ananasbay anginghay ownday omfray eestray.\" \n",
      "and\n",
      "  \" unchesbay ofway eengray ananasbay anginghay outway ybay eestray .\" \n",
      " is: 0.5133450480401704\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway ogsday avehay away ayfulplay ightfay ithway oneway anotherway.\" \n",
      "and\n",
      "  \" otway ogsday avehay away ithway oneway anotherway .\" \n",
      " is: 0.5384952356064083\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Ananabay eestray ithway argelay  anginghay unchesbay ofway ananasbay.\" \n",
      "and\n",
      "  \" eestray avehay argelay inedlay ithway argelay inedlay upway .\" \n",
      " is: 0.48549177170732344\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Erethay areway omesay eengray ananasbay anginghay inway unchesbay\" \n",
      "and\n",
      "  \" erethay areway omesay eengray ananasbay anginghay inway unchesbay\" \n",
      " is: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLUE score (ArgMAX inference): 0.606083\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, sent in enumerate(val_sentences): \n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(sent.lower()) + [\"<EOS>\"]\n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "    output_sentence, _ = inference(input_good, encoder, decoder, decoderType=\"AttentionLSTM\")\n",
    "    target_sentence = piglatin_val_sentences[idx]\n",
    "    score = compute_bleu(target_sentence, output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 10 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "    \n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLUE score (ArgMAX inference): %f\" % (final_score)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvKl0zhhngVg"
   },
   "source": [
    "## 2.5. Visualize Attention for Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsAfscbVnhF6"
   },
   "outputs": [],
   "source": [
    "input_tok  = word_tokenize(val_sentences[0].lower())\n",
    "input_good = [word for word in input_tok if word in word2index]\n",
    "output_sentence, attentions = inference(input_good, encoder, decoder, decoderType=\"AttentionLSTM\")\n",
    "showAttention(val_sentences[0], output_sentence, attentions[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XABkHOBJns76"
   },
   "source": [
    "# Part 3: Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFb5tCzOpDmB"
   },
   "source": [
    "## 3.1 Implement Scaled Dot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EDgSOIvdpCuR"
   },
   "outputs": [],
   "source": [
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x (k))\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, hidden_size = keys.size()\n",
    "        q = self.Q(queries.view(-1, hidden_size)).view(batch_size, -1, hidden_size) \n",
    "        k = self.K(keys.view(-1,hidden_size)).view(batch_size, seq_len, hidden_size) \n",
    "        v = self.V(values.view(-1, hidden_size)).view(batch_size, seq_len, hidden_size) \n",
    "        unnormalized_attention = self.scaling_factor * torch.bmm(k, q.transpose(1,2)) \n",
    "        attention_weights = self.softmax(unnormalized_attention) \n",
    "        context = torch.bmm(attention_weights.transpose(1,2), v) \n",
    "\n",
    "        return context, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4gpSwDopbHZ"
   },
   "source": [
    "## 3.2. Implement Causal Scaled Dot Attention\n",
    "\n",
    "The implementation should be nearly identical to the one above, but with mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZFnGpHklpbpQ"
   },
   "outputs": [],
   "source": [
    "class CausalScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.neg_inf = torch.tensor(-1e7).cuda()\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        NOTES:\n",
    "            batch_size = 1\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "                In training k = maxSequenceLength or length of the GT ourput sequence\n",
    "                In testing k = length of currently decoded sub-sequence\n",
    "            keys: The decoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The decoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, hidden_size = keys.size()\n",
    "        q = self.Q(queries.view(-1, hidden_size)).view(batch_size, -1, hidden_size) \n",
    "        k = self.K(keys.view(-1,hidden_size)).view(batch_size, seq_len, hidden_size) \n",
    "        v = self.V(values.view(-1, hidden_size)).view(batch_size, seq_len, hidden_size) \n",
    "        unnormalized_attention = self.scaling_factor * torch.bmm(k, q.transpose(1,2))\n",
    "        mask = torch.tril(torch.ones(batch_size,seq_len,seq_len,dtype=torch.uint8)).transpose(1,2).cuda()\n",
    "        unnormalized_attention[mask==0] = self.neg_inf\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(1,2), v)\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsLrprNLqQ5x"
   },
   "source": [
    "## 3.3. Implement Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FjiwfUHXqRXI"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = 300\n",
    "        wordEncodingSize = 2000\n",
    "        self.dropout_p = 0.1\n",
    "        self.num_layers = 3\n",
    "        self.linear_input = nn.Linear(wordEncodingSize, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
    "                                    hidden_size=self.hidden_dim, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=self.hidden_dim, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                                    nn.ReLU(),\n",
    "                                 ) for i in range(self.num_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocabularySize)\n",
    "\n",
    "    def forward(self, input_sentence, hidden, cell, annotations):\n",
    "        embed = self.dropout(self.linear_input(input_sentence)).unsqueeze(0)\n",
    "        \n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        batch_size, seq_len, hidden_size = contexts.size()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # ------------\n",
    "            # Your code goes here\n",
    "            # ------------\n",
    "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
    "            residual_contexts = contexts + new_contexts\n",
    "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts,annotations.unsqueeze(0), annotations.unsqueeze(0))\n",
    "            residual_contexts = residual_contexts + new_contexts\n",
    "            new_contexts = self.attention_mlps[i](residual_contexts.view(-1, self.hidden_dim)).view(batch_size, seq_len, self.hidden_dim)\n",
    "            contexts = residual_contexts + new_contexts\n",
    "            \n",
    "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "            self_attention_weights_list.append(self_attention_weights)            \n",
    "        \n",
    "        output = self.linear(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "        \n",
    "        return output, encoder_attention_weights, self_attention_weights\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENolyKTWrAWq"
   },
   "source": [
    "## 3.4. Training Transformer Decoder\n",
    "\n",
    "Note that you will need to modify the train() procedure for Part 1 to handle the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iuUnzkZrA44",
    "outputId": "8415fb3c-60e4-4269-9d6e-acf13fde2e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training end to end network ......\n",
      "Single sentence Loss (epoch 0) : 0.402856\n",
      "Single sentence Loss (epoch 0) : 0.167575\n",
      "Single sentence Loss (epoch 0) : 0.268316\n",
      "Single sentence Loss (epoch 0) : 0.209024\n",
      "Single sentence Loss (epoch 0) : 0.207411\n",
      "Single sentence Loss (epoch 0) : 0.099552\n",
      "Single sentence Loss (epoch 0) : 0.271370\n",
      "Single sentence Loss (epoch 0) : 0.298923\n",
      "Single sentence Loss (epoch 0) : 0.216134\n",
      "Single sentence Loss (epoch 0) : 0.063290\n",
      "Single sentence Loss (epoch 0) : 0.082632\n",
      "Single sentence Loss (epoch 0) : 0.105834\n",
      "Single sentence Loss (epoch 0) : 0.176621\n",
      "Single sentence Loss (epoch 0) : 0.000790\n",
      "Single sentence Loss (epoch 0) : 0.122459\n",
      "Single sentence Loss (epoch 0) : 0.082468\n",
      "Single sentence Loss (epoch 0) : 0.158387\n",
      "Single sentence Loss (epoch 0) : 0.031021\n",
      "Single sentence Loss (epoch 0) : 0.117150\n",
      "Single sentence Loss (epoch 0) : 0.139390\n",
      "Single sentence Loss (epoch 0) : 0.054803\n",
      "Single sentence Loss (epoch 0) : 0.050525\n",
      "Single sentence Loss (epoch 0) : 0.010401\n",
      "Single sentence Loss (epoch 0) : 0.121118\n",
      "Single sentence Loss (epoch 0) : 0.041995\n",
      "Single sentence Loss (epoch 0) : 0.073520\n",
      "Single sentence Loss (epoch 0) : 0.025995\n",
      "Single sentence Loss (epoch 0) : 0.048072\n",
      "Single sentence Loss (epoch 0) : 0.024181\n",
      "Single sentence Loss (epoch 0) : 0.073362\n",
      "Single sentence Loss (epoch 0) : 0.034699\n",
      "Single sentence Loss (epoch 0) : 0.013640\n",
      "Single sentence Loss (epoch 0) : 0.000665\n",
      "Single sentence Loss (epoch 0) : 0.018519\n",
      "Single sentence Loss (epoch 0) : 0.017854\n",
      "Single sentence Loss (epoch 0) : 0.005729\n",
      "Single sentence Loss (epoch 0) : 0.001576\n",
      "Single sentence Loss (epoch 0) : 0.027935\n",
      "Single sentence Loss (epoch 0) : 0.007313\n",
      "Single sentence Loss (epoch 0) : 0.062498\n",
      "Loss (epoch 0) : 0.084088\n",
      "Single sentence Loss (epoch 1) : 0.001339\n",
      "Single sentence Loss (epoch 1) : 0.000051\n",
      "Single sentence Loss (epoch 1) : 0.002098\n",
      "Single sentence Loss (epoch 1) : 0.032838\n",
      "Single sentence Loss (epoch 1) : 0.001894\n",
      "Single sentence Loss (epoch 1) : 0.000075\n",
      "Single sentence Loss (epoch 1) : 0.050322\n",
      "Single sentence Loss (epoch 1) : 0.029223\n",
      "Single sentence Loss (epoch 1) : 0.000286\n",
      "Single sentence Loss (epoch 1) : 0.005156\n",
      "Single sentence Loss (epoch 1) : 0.014857\n",
      "Single sentence Loss (epoch 1) : 0.000631\n",
      "Single sentence Loss (epoch 1) : 0.001162\n",
      "Single sentence Loss (epoch 1) : 0.000845\n",
      "Single sentence Loss (epoch 1) : 0.050369\n",
      "Single sentence Loss (epoch 1) : 0.032548\n",
      "Single sentence Loss (epoch 1) : 0.001991\n",
      "Single sentence Loss (epoch 1) : 0.010690\n",
      "Single sentence Loss (epoch 1) : 0.000151\n",
      "Single sentence Loss (epoch 1) : 0.029358\n",
      "Single sentence Loss (epoch 1) : 0.002385\n",
      "Single sentence Loss (epoch 1) : 0.004910\n",
      "Single sentence Loss (epoch 1) : 0.012550\n",
      "Single sentence Loss (epoch 1) : 0.018081\n",
      "Single sentence Loss (epoch 1) : 0.039792\n",
      "Single sentence Loss (epoch 1) : 0.007442\n",
      "Single sentence Loss (epoch 1) : 0.001752\n",
      "Single sentence Loss (epoch 1) : 0.007268\n",
      "Single sentence Loss (epoch 1) : 0.000875\n",
      "Single sentence Loss (epoch 1) : 0.010386\n",
      "Single sentence Loss (epoch 1) : 0.000085\n",
      "Single sentence Loss (epoch 1) : 0.000827\n",
      "Single sentence Loss (epoch 1) : 0.005813\n",
      "Single sentence Loss (epoch 1) : 0.000197\n",
      "Single sentence Loss (epoch 1) : 0.001519\n",
      "Single sentence Loss (epoch 1) : 0.001029\n",
      "Single sentence Loss (epoch 1) : 0.000018\n",
      "Single sentence Loss (epoch 1) : 0.000736\n",
      "Single sentence Loss (epoch 1) : 0.037706\n",
      "Single sentence Loss (epoch 1) : 0.014557\n",
      "Loss (epoch 1) : 0.012622\n",
      "Single sentence Loss (epoch 2) : 0.000655\n",
      "Single sentence Loss (epoch 2) : 0.000062\n",
      "Single sentence Loss (epoch 2) : 0.000038\n",
      "Single sentence Loss (epoch 2) : 0.026470\n",
      "Single sentence Loss (epoch 2) : 0.000011\n",
      "Single sentence Loss (epoch 2) : 0.000058\n",
      "Single sentence Loss (epoch 2) : 0.049764\n",
      "Single sentence Loss (epoch 2) : 0.001002\n",
      "Single sentence Loss (epoch 2) : 0.000013\n",
      "Single sentence Loss (epoch 2) : 0.000032\n",
      "Single sentence Loss (epoch 2) : 0.022036\n",
      "Single sentence Loss (epoch 2) : 0.000196\n",
      "Single sentence Loss (epoch 2) : 0.000284\n",
      "Single sentence Loss (epoch 2) : 0.000039\n",
      "Single sentence Loss (epoch 2) : 0.031029\n",
      "Single sentence Loss (epoch 2) : 0.143151\n",
      "Single sentence Loss (epoch 2) : 0.001633\n",
      "Single sentence Loss (epoch 2) : 0.001478\n",
      "Single sentence Loss (epoch 2) : 0.000791\n",
      "Single sentence Loss (epoch 2) : 0.010237\n",
      "Single sentence Loss (epoch 2) : 0.000627\n",
      "Single sentence Loss (epoch 2) : 0.000796\n",
      "Single sentence Loss (epoch 2) : 0.000118\n",
      "Single sentence Loss (epoch 2) : 0.009512\n",
      "Single sentence Loss (epoch 2) : 0.005607\n",
      "Single sentence Loss (epoch 2) : 0.000006\n",
      "Single sentence Loss (epoch 2) : 0.000076\n",
      "Single sentence Loss (epoch 2) : 0.000180\n",
      "Single sentence Loss (epoch 2) : 0.000212\n",
      "Single sentence Loss (epoch 2) : 0.012697\n",
      "Single sentence Loss (epoch 2) : 0.000033\n",
      "Single sentence Loss (epoch 2) : 0.000190\n",
      "Single sentence Loss (epoch 2) : 0.000009\n",
      "Single sentence Loss (epoch 2) : 0.002135\n",
      "Single sentence Loss (epoch 2) : 0.001084\n",
      "Single sentence Loss (epoch 2) : 0.000647\n",
      "Single sentence Loss (epoch 2) : 0.000145\n",
      "Single sentence Loss (epoch 2) : 0.000167\n",
      "Single sentence Loss (epoch 2) : 0.000257\n",
      "Single sentence Loss (epoch 2) : 0.099649\n",
      "Loss (epoch 2) : 0.008590\n",
      "Single sentence Loss (epoch 3) : 0.000088\n",
      "Single sentence Loss (epoch 3) : 0.000007\n",
      "Single sentence Loss (epoch 3) : 0.000550\n",
      "Single sentence Loss (epoch 3) : 0.005904\n",
      "Single sentence Loss (epoch 3) : 0.000148\n",
      "Single sentence Loss (epoch 3) : 0.000005\n",
      "Single sentence Loss (epoch 3) : 0.101645\n",
      "Single sentence Loss (epoch 3) : 0.000436\n",
      "Single sentence Loss (epoch 3) : 0.001774\n",
      "Single sentence Loss (epoch 3) : 0.000137\n",
      "Single sentence Loss (epoch 3) : 0.015880\n",
      "Single sentence Loss (epoch 3) : 0.000240\n",
      "Single sentence Loss (epoch 3) : 0.058480\n",
      "Single sentence Loss (epoch 3) : 0.000050\n",
      "Single sentence Loss (epoch 3) : 0.005333\n",
      "Single sentence Loss (epoch 3) : 0.000130\n",
      "Single sentence Loss (epoch 3) : 0.017815\n",
      "Single sentence Loss (epoch 3) : 0.000536\n",
      "Single sentence Loss (epoch 3) : 0.000056\n",
      "Single sentence Loss (epoch 3) : 0.011239\n",
      "Single sentence Loss (epoch 3) : 0.000011\n",
      "Single sentence Loss (epoch 3) : 0.000377\n",
      "Single sentence Loss (epoch 3) : 0.000005\n",
      "Single sentence Loss (epoch 3) : 0.009742\n",
      "Single sentence Loss (epoch 3) : 0.001374\n",
      "Single sentence Loss (epoch 3) : 0.000005\n",
      "Single sentence Loss (epoch 3) : 0.000020\n",
      "Single sentence Loss (epoch 3) : 0.000405\n",
      "Single sentence Loss (epoch 3) : 0.000081\n",
      "Single sentence Loss (epoch 3) : 0.003029\n",
      "Single sentence Loss (epoch 3) : 0.000026\n",
      "Single sentence Loss (epoch 3) : 0.000044\n",
      "Single sentence Loss (epoch 3) : 0.000019\n",
      "Single sentence Loss (epoch 3) : 0.000041\n",
      "Single sentence Loss (epoch 3) : 0.001047\n",
      "Single sentence Loss (epoch 3) : 0.001827\n",
      "Single sentence Loss (epoch 3) : 0.000007\n",
      "Single sentence Loss (epoch 3) : 0.000427\n",
      "Single sentence Loss (epoch 3) : 0.000014\n",
      "Single sentence Loss (epoch 3) : 0.069929\n",
      "Loss (epoch 3) : 0.006960\n",
      "Single sentence Loss (epoch 4) : 0.000155\n",
      "Single sentence Loss (epoch 4) : 0.000018\n",
      "Single sentence Loss (epoch 4) : 0.000418\n",
      "Single sentence Loss (epoch 4) : 0.005091\n",
      "Single sentence Loss (epoch 4) : 0.000035\n",
      "Single sentence Loss (epoch 4) : 0.000043\n",
      "Single sentence Loss (epoch 4) : 0.049541\n",
      "Single sentence Loss (epoch 4) : 0.000018\n",
      "Single sentence Loss (epoch 4) : 0.000004\n",
      "Single sentence Loss (epoch 4) : 0.000054\n",
      "Single sentence Loss (epoch 4) : 0.004820\n",
      "Single sentence Loss (epoch 4) : 0.000004\n",
      "Single sentence Loss (epoch 4) : 0.000227\n",
      "Single sentence Loss (epoch 4) : 0.000054\n",
      "Single sentence Loss (epoch 4) : 0.018422\n",
      "Single sentence Loss (epoch 4) : 0.000447\n",
      "Single sentence Loss (epoch 4) : 0.000465\n",
      "Single sentence Loss (epoch 4) : 0.000185\n",
      "Single sentence Loss (epoch 4) : 0.000380\n",
      "Single sentence Loss (epoch 4) : 0.015911\n",
      "Single sentence Loss (epoch 4) : 0.000014\n",
      "Single sentence Loss (epoch 4) : 0.012210\n",
      "Single sentence Loss (epoch 4) : 0.000008\n",
      "Single sentence Loss (epoch 4) : 0.000162\n",
      "Single sentence Loss (epoch 4) : 0.001220\n",
      "Single sentence Loss (epoch 4) : 0.000004\n",
      "Single sentence Loss (epoch 4) : 0.000586\n",
      "Single sentence Loss (epoch 4) : 0.002606\n",
      "Single sentence Loss (epoch 4) : 0.097272\n",
      "Single sentence Loss (epoch 4) : 0.003409\n",
      "Single sentence Loss (epoch 4) : 0.000007\n",
      "Single sentence Loss (epoch 4) : 0.000331\n",
      "Single sentence Loss (epoch 4) : 0.000037\n",
      "Single sentence Loss (epoch 4) : 0.000160\n",
      "Single sentence Loss (epoch 4) : 0.004981\n",
      "Single sentence Loss (epoch 4) : 0.000081\n",
      "Single sentence Loss (epoch 4) : 0.000004\n",
      "Single sentence Loss (epoch 4) : 0.000065\n",
      "Single sentence Loss (epoch 4) : 0.000058\n",
      "Single sentence Loss (epoch 4) : 0.002558\n",
      "Loss (epoch 4) : 0.006424\n"
     ]
    }
   ],
   "source": [
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder = EncoderLSTM()\n",
    "decoder = TransformerDecoder()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0005) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0005) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "print(\"Start training end to end network ......\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    count=0\n",
    "    for id, sentence in enumerate(filtered_sentences):\n",
    "        target_variable = piglatin_filtered_sentences[id]\n",
    "        loss = train(sentence, target_variable, encoder, decoder,encoder_optimizer,decoder_optimizer, criterion, teacher_forcing_ratio = 1, decoderType=\"Transformer\")\n",
    "        count = count+1\n",
    "        if count%500==0:\n",
    "            print(\"Single sentence Loss (epoch %d) : %f\" % (epoch, loss))\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "    print(\"Loss (epoch %d) : %f\" % (epoch, np.sum(epoch_loss)/len(filtered_sentences))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOF6M3tWrbOS"
   },
   "source": [
    "## 3.5. Testing Transformer Decoder\n",
    "Note that you will need to modify the inference() procedure for Part 1 to handle Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2BlDGfcroOA",
    "outputId": "85d89d7d-92eb-42a7-b669-a40c0d9c3ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score distance between \n",
      "  \"Etsay ofway ananasbay anginghay offway ofway away ananabay eetray.\" \n",
      "and\n",
      "  \" etsay ofway ananasbay anginghay offway ofway away ananabay eetray .\" \n",
      " is: 1.0\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway unchesbay ofway eengray ananasbay onway ananabay eestray.\" \n",
      "and\n",
      "  \" otway earsbay ofway eengray ananasbay onway ananabay eestray onway ananabay eestray .\" \n",
      " is: 0.5331675363405771\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Anymay alendarscay andway unchesbay ofway ananasbay anginghay onway away allway.\" \n",
      "and\n",
      "  \" anymay andway ogethertay away allway ofway ananasbay anginghay onway away allway .\" \n",
      " is: 0.5491004867761125\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Ustersclay ofway ananasbay andway icturespay anginghay onway away allway.\" \n",
      "and\n",
      "  \" ofway ananasbay andway icturespay anginghay onway away allway .\" \n",
      " is: 0.8948393168143697\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"otway ogsday atthay ooklay otay ebay ightingfay oneway anotherway\" \n",
      "and\n",
      "  \" otway ogsday atthay ooklay otay ebay oneway anotherway\" \n",
      " is: 0.6752918218126556\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway ogsday ightingfay ithway oneway onway ishay ackbay onway ethay oundgray\" \n",
      "and\n",
      "  \" otway ogsday ithway oneway onway ishay ackbay onway ethay oundgray\" \n",
      " is: 0.7516501147964686\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Unchesbay ofway eengray ananasbay anginghay ownday omfray eestray.\" \n",
      "and\n",
      "  \" atuestay ofway eengray ananasbay anginghay ownday omfray eestray anginghay ownday omfray eestray .\" \n",
      " is: 0.5593684915933074\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Otway ogsday avehay away ayfulplay ightfay ithway oneway anotherway.\" \n",
      "and\n",
      "  \" otway ogsday avehay away ithway oneway anotherway .\" \n",
      " is: 0.5384952356064083\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Ananabay eestray ithway argelay  anginghay unchesbay ofway ananasbay.\" \n",
      "and\n",
      "  \" ananabay eestray ithway argelay anginghay onway argelay anginghay ogethertay ofway ananasbay .\" \n",
      " is: 0.4240125351805037\n",
      "\n",
      "\n",
      "BLEU score distance between \n",
      "  \"Erethay areway omesay eengray ananasbay anginghay inway unchesbay\" \n",
      "and\n",
      "  \" erethay areway omesay eengray ananasbay anginghay inway ogethertay inway asesvay\" \n",
      " is: 0.6389431042462724\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLUE score (ArgMAX inference): 0.736126\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "avg_score=[]\n",
    "\n",
    "# iterate over the validation set \n",
    "for idx, sent in enumerate(val_sentences): \n",
    "    input_tok = [\"<SOS>\"] + word_tokenize(sent.lower()) + [\"<EOS>\"]\n",
    "    input_good = [word for word in input_tok if word in word2index]\n",
    "    output_sentence, _ = inference(input_good, encoder, decoder, decoderType=\"Transformer\")\n",
    "    target_sentence = piglatin_val_sentences[idx]\n",
    "    score = compute_bleu(target_sentence, output_sentence)\n",
    "    avg_score.append(score)\n",
    "    if idx < 10 :\n",
    "        print('BLEU score distance between \\n  \"' + target_sentence + '\" \\nand\\n  \"'+ output_sentence + '\" \\n is: ' + str(score) +'\\n\\n')\n",
    "    \n",
    "final_score = np.sum(avg_score)/len(val_sentences)\n",
    "print(\"Average BLUE score (ArgMAX inference): %f\" % (final_score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCrOGW2NrzPG"
   },
   "source": [
    "## 3.6 Visualizing Attention for Transformer Decoder\n",
    "\n",
    "Note that since we have multiple attention layers, there will be one attention to be visualized per layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875
    },
    "id": "ryIRDG9UC6-G",
    "outputId": "162cafa9-010b-4220-a5b3-91f80c9e9f94"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sentence = val_sentences[0]\n",
    "input_tok = [\"<SOS>\"] + word_tokenize(input_sentence.lower()) + [\"<EOS>\"]\n",
    "input_good = [word for word in input_tok if word in word2index]\n",
    "output_sentence, encoder_attentions = inference(input_good, encoder, decoder, decoderType=\"Transformer\")\n",
    "encoder_attentions = encoder_attentions.data.cpu()\n",
    "\n",
    "for i in range(encoder_attentions.shape[0]):\n",
    "    showAttention(input_sentence, output_sentence, encoder_attentions[i,0,:15,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jng5Ksadkdp"
   },
   "source": [
    "# 4. Effectiveness of word2vec\n",
    "\n",
    "As an option, you may repeat one of the models above by modifying the code to use word2vec embedding for the input English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RWVVvgyudkdq",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CPSC532S_Assignment3_solution_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
